{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35TOzJw2CpLA"
   },
   "source": [
    "### COMP3359 Final Project: Toxic Comment Classification\n",
    "\n",
    "\n",
    "## Objective\n",
    "There are enormous discussions happening in our social media everyday and just one toxic comment can sour the whole discussion. Many platforms are struggling to effectively keep the environment clean. And how to define toxic may partly depend on what the platform is. Like some legal adult websites may be ok with obscene, but most are not. \n",
    "\n",
    "Therefore, the artificial intelligence program will not only detect the toxicity of the comment, but also label the comment with its toxicity type if it is. \n",
    "\n",
    "\n",
    "## Overview \n",
    "[Start With Simple LSTM Model](#p1) \n",
    "\n",
    "[Experiment With Other Models](#p2) \n",
    " \n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wfbr8Iuy9XHf"
   },
   "source": [
    "<a id=’p1’></a>\n",
    "# Start With Simple LSTM Model\n",
    "\n",
    "This section implemented a model with embedding+LSTM layer + 1 dense layer with dropout.\n",
    "\n",
    "Model Hyperparameters to test:\n",
    "1. [Sentence Max Length: 100, 200, ](#p11) \n",
    "2. [Word Embedding Dimension: 64, 128, 256](#p12) \n",
    "3. [Batch size: 500, 1000, 5000](#p13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 781,
     "status": "ok",
     "timestamp": 1588546474133,
     "user": {
      "displayName": "Yuyang Lin",
      "photoUrl": "",
      "userId": "11037694261943317117"
     },
     "user_tz": -480
    },
    "id": "AeIUlwdKpg5Y",
    "outputId": "ba383fd7-c035-46a4-ace2-8edff7d0edb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# this is optional (actually not necessary at all, who else will use google's laggy service)\n",
    "\n",
    "\"\"\" Prepare Notebook for Google Colab \"\"\"\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Specify directory of course materials in Google Drive\n",
    "module_dir = '/content/drive/My Drive/3359proj/'\n",
    "\n",
    "# Add course material directory in Google Drive to system path, for importing .py files later\n",
    "# (Ref.: https://stackoverflow.com/questions/48905127/importing-py-files-in-google-colab)\n",
    "import sys\n",
    "sys.path.append(module_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3910,
     "status": "ok",
     "timestamp": 1588546480813,
     "user": {
      "displayName": "Yuyang Lin",
      "photoUrl": "",
      "userId": "11037694261943317117"
     },
     "user_tz": -480
    },
    "id": "Uu1ND_X_pQwH",
    "outputId": "65e26718-5226-4acc-800d-7dd7856c7c98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.3)\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3692,
     "status": "ok",
     "timestamp": 1588546486399,
     "user": {
      "displayName": "Yuyang Lin",
      "photoUrl": "",
      "userId": "11037694261943317117"
     },
     "user_tz": -480
    },
    "id": "kK5tVs8RpQwO",
    "outputId": "91158d2b-3950-47e2-fed3-c108fe28c2e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Load Data \"\"\"\n",
    "\n",
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, Dropout, Flatten, MaxPooling1D, Concatenate, GlobalMaxPooling1D\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "# # this is for google colab( DONOT RUN IT if working locally)\n",
    "data_dir = os.path.join(module_dir, \"input/\")\n",
    "data_path_train = os.path.join(data_dir, \"train.csv\")\n",
    "data_path_test = os.path.join(data_dir, \"test.csv\")\n",
    "data_path_test_label = os.path.join(data_dir, \"test_labels.csv\")\n",
    "train = pd.read_csv(data_path_train)\n",
    "test = pd.read_csv(data_path_test)\n",
    "test_label = pd.read_csv(data_path_test_label)\n",
    "\n",
    "# train = pd.read_csv('./input/train.csv')\n",
    "# test = pd.read_csv('./input/test.csv')\n",
    "# test_label = pd.read_csv('./input/test_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1015,
     "status": "ok",
     "timestamp": 1588497084602,
     "user": {
      "displayName": "Yuyang Lin",
      "photoUrl": "",
      "userId": "11037694261943317117"
     },
     "user_tz": -480
    },
    "id": "fYF50COQ2x0H",
    "outputId": "a1f7eeb1-cfde-4fcd-fa28-8eb27d2c7559"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(id               False\n",
       " comment_text     False\n",
       " toxic            False\n",
       " severe_toxic     False\n",
       " obscene          False\n",
       " threat           False\n",
       " insult           False\n",
       " identity_hate    False\n",
       " dtype: bool, id              False\n",
       " comment_text    False\n",
       " dtype: bool)"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the data has the null input if so, do some data engineering\n",
    "train.isnull().any(),test.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8W-xoZz0pQwY"
   },
   "outputs": [],
   "source": [
    "sen_train = train[\"comment_text\"]\n",
    "sen_test_temp = test[\"comment_text\"]\n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y_train = train[list_classes].values\n",
    "y_test_temp = test_label[list_classes].values\n",
    "\n",
    "sen_test = []\n",
    "y_test = []\n",
    "\n",
    "test_no = len(y_test_temp)\n",
    "for i in range(test_no):\n",
    "  if not np.array_equal(y_test_temp[i], [-1,-1,-1,-1,-1,-1]):\n",
    "    y_test.append(y_test_temp[i])\n",
    "    sen_test.append(sen_test_temp[i])\n",
    "    \n",
    "y_test = np.array(y_test)\n",
    "sen_test = np.array(sen_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rCyKJy_ZpQwd"
   },
   "outputs": [],
   "source": [
    "# Vocabulary size for tokenization\n",
    "vocab_size = 20000\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(list(sen_train))\n",
    "tokenized_train = tokenizer.texts_to_sequences(sen_train)\n",
    "tokenized_test = tokenizer.texts_to_sequences(sen_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XcOn-q5ryHvh"
   },
   "outputs": [],
   "source": [
    "# as usually DON'T RUN it in order to save your time\n",
    "totalNumWords = [len(one_comment) for one_comment in tokenized_train]\n",
    "plt.hist(totalNumWords,bins = np.arange(0,410,10))#[0,50,100,150,200,250,300,350,400])#,450,500,550,600,650,700,750,800,850,900])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TghmEOmDpQwg"
   },
   "outputs": [],
   "source": [
    "max_length200 = 200\n",
    "max_length150 = 150\n",
    "max_length100 = 100\n",
    "batch_size500 = 500\n",
    "batch_size50 = 50\n",
    "epochs = 15\n",
    "emd_size = 128\n",
    "emd_size64 = 64\n",
    "emd_size256 = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KJ1X1gpPpQwi"
   },
   "source": [
    "build a simple modle with LSTM; Skipp the following when running other models as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RMtO4ivXCRF5"
   },
   "source": [
    "<a id=’p11’></a>\n",
    "### 1.  Sentence Max Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5GacajKRq-Dw"
   },
   "source": [
    "case 0.1.1: max length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3204,
     "status": "ok",
     "timestamp": 1588546623769,
     "user": {
      "displayName": "Yuyang Lin",
      "photoUrl": "",
      "userId": "11037694261943317117"
     },
     "user_tz": -480
    },
    "id": "BcDZsA3bpQwj",
    "outputId": "a28ef86a-b343-4326-a3f6-f17ffed755dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200, 60)           45360     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 366       \n",
      "=================================================================\n",
      "Total params: 2,605,726\n",
      "Trainable params: 2,605,726\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x_train = pad_sequences(tokenized_train, maxlen=max_length200)\n",
    "x_test = pad_sequences(tokenized_test, maxlen=max_length200)\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(vocab_size, emd_size, input_length=max_length200,))\n",
    "model_lstm.add(LSTM(units=60,return_sequences = True))\n",
    "model_lstm.add(GlobalMaxPooling1D())\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(Dense(6,activation='sigmoid'))\n",
    "model_lstm.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "print(model_lstm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AuZWH4TUpQwm",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = Model(inputs=inp,outputs=x)\n",
    "# model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "# print(model.summary())\n",
    "#history = model.fit(x_train,y_train, batch_size=batch_size, epochs=epochs, validation_data=[x_test,y_test])\n",
    "\n",
    "# Fit model\n",
    "history = model_lstm.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SDOX48TLCpLo"
   },
   "source": [
    "159571/159571 [==============================] - 89s 556us/step\n",
    "Training accuracy: 0.98535\n",
    "63978/63978 [==============================] - 36s 561us/step\n",
    "Testing Accuracy: 0.96839"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQQXbppJsuxN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss,accuracy = model_lstm.evaluate(x_train,y_train)\n",
    "print(\"Training accuracy: {:.5f}\".format(accuracy))\n",
    "loss,accuracy = model_lstm.evaluate(x_test,y_test)\n",
    "print(\"Testing Accuracy: {:.5f}\".format(accuracy))\n",
    "\n",
    "\"\"\" Visualize Training Results \"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Get training results\n",
    "history_dict = history.history\n",
    "train_acc = history_dict['accuracy']\n",
    "test_acc = history_dict['val_accuracy']\n",
    "\n",
    "# Plot training results\n",
    "plt.plot(train_acc, label='Train Acc.')\n",
    "plt.plot(test_acc, label='Test Acc.')\n",
    "\n",
    "# Show plot\n",
    "plt.title('Model Training Results(maxlen=200,batch_size=1000,emd_size=64)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nt7OryT5rEis"
   },
   "source": [
    "case 0.1.2: max length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qEnMqLuMHkzB"
   },
   "outputs": [],
   "source": [
    "x_train = pad_sequences(tokenized_train, maxlen=max_length100)\n",
    "x_test = pad_sequences(tokenized_test, maxlen=max_length100)\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(vocab_size, emd_size, input_length=max_length100,))\n",
    "model_lstm.add(LSTM(units=60,return_sequences = True))\n",
    "model_lstm.add(GlobalMaxPooling1D())\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(Dense(6,activation='sigmoid'))\n",
    "model_lstm.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "history = model_lstm.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8-dCSbOtjf6G"
   },
   "outputs": [],
   "source": [
    "loss,accuracy = model_lstm.evaluate(x_train,y_train)\n",
    "print(\"Training accuracy: {:.5f}\".format(accuracy))\n",
    "loss,accuracy = model_lstm.evaluate(x_test,y_test)\n",
    "print(\"Testing Accuracy: {:.5f}\".format(accuracy))\n",
    "\n",
    "\"\"\" Visualize Training Results \"\"\"\n",
    "# Get training results\n",
    "history_dict = history.history\n",
    "train_acc = history_dict['accuracy']\n",
    "test_acc = history_dict['val_accuracy']\n",
    "\n",
    "# Plot training results\n",
    "plt.plot(train_acc, label='Train Acc.')\n",
    "plt.plot(test_acc, label='Test Acc.')\n",
    "\n",
    "# Show plot\n",
    "plt.title('Model Training Results(maxlen=100)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UQFLPtQprM8z"
   },
   "source": [
    "case 0.1.3: max length = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rXsHXJjKjSlZ"
   },
   "outputs": [],
   "source": [
    "x_train = pad_sequences(tokenized_train, maxlen=max_length150) # if 100 still overfit change to 50\n",
    "x_test = pad_sequences(tokenized_test, maxlen=max_length150)\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(vocab_size, emd_size, input_length=max_length300,))\n",
    "model_lstm.add(LSTM(units=60,return_sequences = True))\n",
    "model_lstm.add(GlobalMaxPooling1D())\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(Dense(6,activation='sigmoid'))\n",
    "model_lstm.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = model_lstm.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NJCMjD5Vjji7"
   },
   "outputs": [],
   "source": [
    "loss,accuracy = model_lstm.evaluate(x_train,y_train)\n",
    "print(\"Training accuracy: {:.5f}\".format(accuracy))\n",
    "loss,accuracy = model_lstm.evaluate(x_test,y_test)\n",
    "print(\"Testing Accuracy: {:.5f}\".format(accuracy))\n",
    "\n",
    "\"\"\" Visualize Training Results \"\"\"\n",
    "# Get training results\n",
    "history_dict = history.history\n",
    "train_acc = history_dict['accuracy']\n",
    "test_acc = history_dict['val_accuracy']\n",
    "\n",
    "# Plot training results\n",
    "plt.plot(train_acc, label='Train Acc.')\n",
    "plt.plot(test_acc, label='Test Acc.')\n",
    "\n",
    "# Show plot\n",
    "plt.title('Model Training Results(maxlen=300)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CT1DsuilCimA"
   },
   "source": [
    "<a id=’p12’></a>\n",
    "### 2. Word Embedding Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "REYhfzkhrVs8"
   },
   "source": [
    "case 0.2.1: emd size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iiuaujck0KIG"
   },
   "outputs": [],
   "source": [
    "x_train = pad_sequences(tokenized_train, maxlen=max_length200)\n",
    "x_test = pad_sequences(tokenized_test, maxlen=max_length200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ui1DE_j602Ys"
   },
   "outputs": [],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(vocab_size, emd_size128, input_length=max_length200,))\n",
    "model_lstm.add(LSTM(units=60,return_sequences = True))\n",
    "model_lstm.add(GlobalMaxPooling1D())\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(Dense(6,activation='sigmoid'))\n",
    "model_lstm.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = model_lstm.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_EyPCivMldfU"
   },
   "outputs": [],
   "source": [
    "loss,accuracy = model_lstm.evaluate(x_train,y_train)\n",
    "print(\"Training accuracy: {:.5f}\".format(accuracy))\n",
    "loss,accuracy = model_lstm.evaluate(x_test,y_test)\n",
    "print(\"Testing Accuracy: {:.5f}\".format(accuracy))\n",
    "\n",
    "\"\"\" Visualize Training Results \"\"\"\n",
    "# Get training results\n",
    "history_dict = history.history\n",
    "train_acc = history_dict['accuracy']\n",
    "test_acc = history_dict['val_accuracy']\n",
    "\n",
    "# Plot training results\n",
    "plt.plot(train_acc, label='Train Acc.')\n",
    "plt.plot(test_acc, label='Test Acc.')\n",
    "\n",
    "# Show plot\n",
    "plt.title('Model Training Results(emd_size=128)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CTVK3WhUrd8b"
   },
   "source": [
    "case 0.2.2: emd size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N4O3ye1X1qqt"
   },
   "outputs": [],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(vocab_size, emd_size256, input_length=max_length200,))\n",
    "model_lstm.add(LSTM(units=60,return_sequences = True))\n",
    "model_lstm.add(GlobalMaxPooling1D())\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(Dense(6,activation='sigmoid'))\n",
    "model_lstm.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = model_lstm.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lMRXSxTa1u3G"
   },
   "outputs": [],
   "source": [
    "loss,accuracy = model_lstm.evaluate(x_train,y_train)\n",
    "print(\"Training accuracy: {:.5f}\".format(accuracy))\n",
    "loss,accuracy = model_lstm.evaluate(x_test,y_test)\n",
    "print(\"Testing Accuracy: {:.5f}\".format(accuracy))\n",
    "\n",
    "\"\"\" Visualize Training Results \"\"\"\n",
    "# Get training results\n",
    "history_dict = history.history\n",
    "train_acc = history_dict['accuracy']\n",
    "test_acc = history_dict['val_accuracy']\n",
    "\n",
    "# Plot training results\n",
    "plt.plot(train_acc, label='Train Acc.')\n",
    "plt.plot(test_acc, label='Test Acc.')\n",
    "\n",
    "# Show plot\n",
    "plt.title('Model Training Results(emd_size=256)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ho3tCJ3aC2BB"
   },
   "source": [
    "<a id=’p13’></a>\n",
    "### 3. Batch Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PGbtMGxDr28o"
   },
   "source": [
    "case 0.3.1: batch size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jNwkUjYylSxW"
   },
   "outputs": [],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(vocab_size, emd_size, input_length=max_length200,))\n",
    "model_lstm.add(LSTM(units=60,return_sequences = True))\n",
    "model_lstm.add(GlobalMaxPooling1D())\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(Dense(6,activation='sigmoid'))\n",
    "model_lstm.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = model_lstm.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HtRCuEU7leRi"
   },
   "outputs": [],
   "source": [
    "loss,accuracy = model_lstm.evaluate(x_train,y_train)\n",
    "print(\"Training accuracy: {:.5f}\".format(accuracy))\n",
    "loss,accuracy = model_lstm.evaluate(x_test,y_test)\n",
    "print(\"Testing Accuracy: {:.5f}\".format(accuracy))\n",
    "\n",
    "\"\"\" Visualize Training Results \"\"\"\n",
    "# Get training results\n",
    "history_dict = history.history\n",
    "train_acc = history_dict['accuracy']\n",
    "test_acc = history_dict['val_accuracy']\n",
    "\n",
    "# Plot training results\n",
    "plt.plot(train_acc, label='Train Acc.')\n",
    "plt.plot(test_acc, label='Test Acc.')\n",
    "\n",
    "# Show plot\n",
    "plt.title('Model Training Results(bs=500)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uQzYDS8Gr6ld"
   },
   "source": [
    "case 0.3.2: batch size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aJPI50GnlTSt"
   },
   "outputs": [],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(vocab_size, emd_size, input_length=max_length200,))\n",
    "model_lstm.add(LSTM(units=60,return_sequences = True))\n",
    "model_lstm.add(GlobalMaxPooling1D())\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(Dense(6,activation='sigmoid'))\n",
    "model_lstm.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = model_lstm.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AMY8nrD3lfTR"
   },
   "outputs": [],
   "source": [
    "loss,accuracy = model_lstm.evaluate(x_train,y_train)\n",
    "print(\"Training accuracy: {:.5f}\".format(accuracy))\n",
    "loss,accuracy = model_lstm.evaluate(x_test,y_test)\n",
    "print(\"Testing Accuracy: {:.5f}\".format(accuracy))\n",
    "\n",
    "\"\"\" Visualize Training Results \"\"\"\n",
    "# Get training results\n",
    "history_dict = history.history\n",
    "train_acc = history_dict['accuracy']\n",
    "test_acc = history_dict['val_accuracy']\n",
    "\n",
    "# Plot training results\n",
    "plt.plot(train_acc, label='Train Acc.')\n",
    "plt.plot(test_acc, label='Test Acc.')\n",
    "\n",
    "# Show plot\n",
    "plt.title('Model Training Results(bs=5000)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "weyQGVAhCpLu"
   },
   "source": [
    "###@Author Pu Hongxi\n",
    "<br><br>\n",
    "\n",
    "<a id=’p2’></a>\n",
    "# Experiment with other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eMcR8x5tsWIV"
   },
   "source": [
    "1. [TextCNN](#p21) \n",
    "2. [CNN+LSTM](#p22) \n",
    "3. [BiDirectional RNN(LSTM/GRU)](#p23)\n",
    "4. [Attention](#p24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aGtcsnJ1pEHT"
   },
   "outputs": [],
   "source": [
    "\"\"\" common variables for all models\"\"\"\n",
    "\n",
    "emd_size = 128 # optimal get from LSTM\n",
    "filters = 250\n",
    "hidden_dims = 64\n",
    "vocab_size = 20000 \n",
    "max_length = 200 # optiaml get from LSTM\n",
    "epochs = 15\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oS3D423-CpLu"
   },
   "source": [
    "<a id=’p21’></a>\n",
    "## 1.TextCNN\n",
    "According to the paper Convolutional Neural Networks for Sentence Classification by Yoon Kim, we can use word vector and use CNN as we do to images. The paper says CNN has excellent performance on sentence-level classification tasks with multiple benchmarks. So let's first give it a shoot and we will do some analysis once we have got the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BFZzGy2QWfy9"
   },
   "source": [
    "Model Hyperparameters to test:\n",
    "\n",
    "1. [Filter Size](#s11) \n",
    "2. [Density Layer](#s12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c9wsgmHJCpLu"
   },
   "outputs": [],
   "source": [
    "#Preprocessing  is the same; Running the following when the padding is finished\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Flatten, MaxPooling1D, Input, Concatenate,Conv2D, MaxPool2D, Reshape, CuDNNLSTM\n",
    "from keras.models import load_model\n",
    "x_train;\n",
    "y_train;\n",
    "x_test;\n",
    "y_test;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vlJrsnqeCpLw"
   },
   "outputs": [],
   "source": [
    "#CNN model\n",
    "#try to choose the same parameter as before so that we can compare the result with regards to the model choice\n",
    "#Parameters without specific comment are subject to fine-tuning.\n",
    "# the reason why I set them to numerical value is that I don't want to re-run to load the parameter value specified before\n",
    "emd_size = 128\n",
    "filters = 250\n",
    "kernal_size = 3 # normally is set to 3\n",
    "hidden_dims = 64\n",
    "vocab_size = 20000 # =max_features\n",
    "max_length = 200 # =maxlen\n",
    "epochs #2 It's better to change it to something larger say at least 5\n",
    "#but I don't want to waste time waiting so I will leave the training with larger epoch to my parter\n",
    "\n",
    "\n",
    "#As below is a shallow CNN; Definitely we can try with another deep CNN but it will take some time to fine-tuning as well hence\n",
    "# I will take care of that if I do have some spare time.\n",
    "# The reset is just routine with nothing worth mentioning.\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Embedding(vocab_size, emd_size, input_length=max_length,))\n",
    "model_cnn.add(Conv1D(filters, kernal_size, activation='relu'))\n",
    "model_cnn.add(GlobalMaxPooling1D())\n",
    "model_cnn.add(Dense(hidden_dims,activation='relu'))\n",
    "model_cnn.add(Dropout(0.5))\n",
    "model_cnn.add(Dense(6,activation='sigmoid'))\n",
    "model_cnn.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model_cnn.summary()\n",
    "\n",
    "#Fit model\n",
    "history = model_cnn.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yHED-QK2CpLy",
    "outputId": "379f519f-0bed-41ae-c749-a4c6c515552a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159571/159571 [==============================] - 27s 169us/step\n",
      "EMM! <manual Exclamation> That's the Training accuracy: 0.98581\n",
      "63978/63978 [==============================] - 11s 172us/step\n",
      "That's the one: Testing Accuracy: 0.97131\n"
     ]
    }
   ],
   "source": [
    "loss,accuracy = model_cnn.evaluate(x_train,y_train)\n",
    "print(\"EMM! <manual Exclamation> That's the Training accuracy: {:.5f}\".format(accuracy))\n",
    "loss,accuracy = model_cnn.evaluate(x_test,y_test)\n",
    "print(\"That's the one: Testing Accuracy: {:.5f}\".format(accuracy))\n",
    "\n",
    "#should I visualize it? No way for such fancy useless things. Do it yourself if you want.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SIavz8vKCpL0"
   },
   "source": [
    "159571/159571 [==============================] - 27s 169us/step\n",
    "EMM! <manual Exclamation> That's the Training accuracy: 0.98581\n",
    "63978/63978 [==============================] - 11s 172us/step\n",
    "That's the one: Testing Accuracy: 0.97131\n",
    "\n",
    "    It has better performance than LSTM along"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gT__hC4yiGWC"
   },
   "source": [
    "<a id=’s11’></a>\n",
    "### 1.1 Filter Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L4vhgMHIjOWi"
   },
   "source": [
    "case 1.1.1: Filter Size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iPAgbriaXy1f"
   },
   "outputs": [],
   "source": [
    "filter_size = 5\n",
    "\n",
    "model_cnn2 = Sequential()\n",
    "model_cnn2.add(Embedding(vocab_size, emd_size, input_length=max_length,))\n",
    "model_cnn2.add(Conv1D(filters, filter_size, activation='relu'))\n",
    "model_cnn2.add(GlobalMaxPooling1D())\n",
    "model_cnn2.add(Dense(hidden_dims,activation='relu'))\n",
    "model_cnn2.add(Dropout(0.5))\n",
    "model_cnn2.add(Dense(6,activation='sigmoid'))\n",
    "model_cnn2.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "print(model_cnn2.summary())\n",
    "\n",
    "#Fit model\n",
    "history = model_cnn2.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size)\n",
    "\n",
    "#Fit model\n",
    "# history = model_cnn3.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jA1JXvP8jiey"
   },
   "outputs": [],
   "source": [
    "loss,accuracy = model_cnn2.evaluate(x_train,y_train)\n",
    "print(\"Training accuracy: {:.5f}\".format(accuracy))\n",
    "loss,accuracy = model_cnn2.evaluate(x_test,y_test)\n",
    "print(\"Testing Accuracy: {:.5f}\".format(accuracy))\n",
    "\n",
    "\"\"\" Visualize Training Results \"\"\"\n",
    "# Get training results\n",
    "history_dict = history.history\n",
    "train_acc = history_dict['accuracy']\n",
    "test_acc = history_dict['val_accuracy']\n",
    "\n",
    "# Plot training results\n",
    "plt.plot(train_acc, label='Train Acc.')\n",
    "plt.plot(test_acc, label='Test Acc.')\n",
    "\n",
    "# Show plot\n",
    "plt.title('Model Training Results(TextCNN: filter=5)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QhddnzFfjSBR"
   },
   "source": [
    "case 1.1.2: Filter Size = [3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "33c48M-YSNbK"
   },
   "outputs": [],
   "source": [
    "filter_sizes = [3,4,5]\n",
    "\n",
    "model_cnn3 = Sequential()\n",
    "model_cnn3.add(Embedding(vocab_size, emd_size, input_length=max_length,))\n",
    "for i in range(len(filter_sizes)):\n",
    "  model_cnn3.add(Conv1D(filters, filter_sizes[i], activation='relu'))\n",
    "\n",
    "model_cnn3.add(GlobalMaxPooling1D())\n",
    "model_cnn3.add(Dense(hidden_dims,activation='relu'))\n",
    "model_cnn3.add(Dropout(0.5))\n",
    "model_cnn3.add(Dense(6,activation='sigmoid'))\n",
    "model_cnn3.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "print(model_cnn3.summary())\n",
    "\n",
    "#Fit model\n",
    "history = model_cnn3.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ljw2UpTyjjd6"
   },
   "outputs": [],
   "source": [
    "loss,accuracy = model_cnn3.evaluate(x_train,y_train)\n",
    "print(\"Training accuracy: {:.5f}\".format(accuracy))\n",
    "loss,accuracy = model_cnn3.evaluate(x_test,y_test)\n",
    "print(\"Testing Accuracy: {:.5f}\".format(accuracy))\n",
    "\n",
    "\"\"\" Visualize Training Results \"\"\"\n",
    "# Get training results\n",
    "history_dict = history.history\n",
    "train_acc = history_dict['accuracy']\n",
    "test_acc = history_dict['val_accuracy']\n",
    "\n",
    "# Plot training results\n",
    "plt.plot(train_acc, label='Train Acc.')\n",
    "plt.plot(test_acc, label='Test Acc.')\n",
    "\n",
    "# Show plot\n",
    "plt.title('Model Training Results(TextCNN: filter=[3,4,5])')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gm5m0Q46Xexf"
   },
   "source": [
    "<a id=’s12’></a>\n",
    "### 1.2 Density Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wwryvn40kW7u"
   },
   "source": [
    "case 1.2.1: For case 1.1.1 remove one density layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_QWCdf_tVTra"
   },
   "outputs": [],
   "source": [
    "filter_size = 5\n",
    "\n",
    "model_cnn4 = Sequential()\n",
    "model_cnn4.add(Embedding(vocab_size, emd_size, input_length=max_length,))\n",
    "model_cnn4.add(Conv1D(filters, filter_size, activation='relu'))\n",
    "model_cnn4.add(GlobalMaxPooling1D())\n",
    "model_cnn4.add(Dropout(0.5))\n",
    "model_cnn4.add(Dense(6,activation='sigmoid'))\n",
    "model_cnn4.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "print(model_cnn4.summary())\n",
    "\n",
    "#Fit model\n",
    "history = model_cnn4.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UcxdEAGPk4Xa"
   },
   "outputs": [],
   "source": [
    "loss,accuracy = model_cnn4.evaluate(x_train,y_train)\n",
    "print(\"Training accuracy: {:.5f}\".format(accuracy))\n",
    "loss,accuracy = model_cnn4.evaluate(x_test,y_test)\n",
    "print(\"Testing Accuracy: {:.5f}\".format(accuracy))\n",
    "\n",
    "\"\"\" Visualize Training Results \"\"\"\n",
    "# Get training results\n",
    "history_dict = history.history\n",
    "train_acc = history_dict['accuracy']\n",
    "test_acc = history_dict['val_accuracy']\n",
    "\n",
    "# Plot training results\n",
    "plt.plot(train_acc, label='Train Acc.')\n",
    "plt.plot(test_acc, label='Test Acc.')\n",
    "\n",
    "# Show plot\n",
    "plt.title('Model Training Results(textCNN: delete middle density layer)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4jcPsJMbCpL0"
   },
   "source": [
    "<a id=’p22’></a>\n",
    "## 2.CNN + LSTM\n",
    "We have tried CNN and LSTM above, now let's implement them together to see the result. The motivation is that I want to see how it handles long sequences together with what to keep and what to forget.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y6tAFuxGCpL0"
   },
   "outputs": [],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, MaxPooling1D,GlobalAveragePooling1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Esdm7e6_CpL2"
   },
   "outputs": [],
   "source": [
    "#CNN model\n",
    "#try to choose the same parameter as before so that we can compare the result with regards to the model choice\n",
    "#Parameters without specific comment are subject to fine-tuning.\n",
    "# the reason why I set them to numerical value is that I don't want to re-run to load the parameter value specified before\n",
    "emd_size = 128\n",
    "filters = 250\n",
    "kernal_size = 3 # normally is set to 3\n",
    "hidden_dims = 128\n",
    "vocab_size = 20000 # =max_features\n",
    "max_length = 200 # =maxlen\n",
    "units=60\n",
    "epochs #2 It's better to change it to something larger say at least 5\n",
    "#but I don't want to waste time waiting so I will leave the training with larger epoch to my parter\n",
    "\n",
    "\n",
    "#As below is a shallow CNN; Definitely we can try with another deep CNN but it will take some time to fine-tuning as well hence\n",
    "# I will take care of that if I do have some spare time.\n",
    "# The reset is just routine with nothing worth mentioning.\n",
    "model_cnn_lstm = Sequential()\n",
    "model_cnn_lstm.add(Embedding(vocab_size, emd_size, input_length=max_length,))\n",
    "model_cnn_lstm.add(Conv1D(filters, kernal_size, activation='relu'))\n",
    "model_cnn_lstm.add(MaxPooling1D()) # also try 2D \n",
    "model_cnn_lstm.summary()\n",
    "model_cnn_lstm.add(LSTM(units=units)) #same as the above\n",
    "model_cnn_lstm.add(Dense(hidden_dims,activation='relu'))\n",
    "model_cnn_lstm.add(Dropout(0.5))\n",
    "model_cnn_lstm.add(Dense(6,activation='sigmoid'))\n",
    "model_cnn_lstm.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model_cnn_lstm.summary()\n",
    "\n",
    "#Fit model\n",
    "history = model_cnn_lstm.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GRzpBezHCpL4",
    "outputId": "3df60961-d1ee-46d1-a079-11da206a65f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159571/159571 [==============================] - 93s 580us/step\n",
      "EMM! <manual Exclamation> That's the Training accuracy: 0.98529\n",
      "63978/63978 [==============================] - 38s 591us/step\n",
      "That's the one: Testing Accuracy: 0.96890\n"
     ]
    }
   ],
   "source": [
    "loss,accuracy = model_cnn_lstm.evaluate(x_train,y_train)\n",
    "print(\"EMM! <manual Exclamation> That's the Training accuracy: {:.5f}\".format(accuracy))\n",
    "loss,accuracy = model_cnn_lstm.evaluate(x_test,y_test)\n",
    "print(\"That's the one: Testing Accuracy: {:.5f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OUl-nAH5YHV9"
   },
   "source": [
    "Combine TextCNN and LSTM after getting the result of part 1 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_uXyrTw5f_zm"
   },
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "70S1R71bCpL6"
   },
   "source": [
    "Till now, the test accuracy of various models is\n",
    "LSTM:  0.96839\n",
    "CNN:  0.97131\n",
    "CNN+ LSTM: 0.96890 \n",
    "\n",
    "IT's improved compared with the original one. \n",
    "As for why it's no better than CNN, I will let my parnter to find out by changing some parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zFoWJFpzCpL7"
   },
   "source": [
    "<a id=’p23’></a>\n",
    "## 3.BiDirectional RNN(LSTM/GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DnXBYUcRCpL8"
   },
   "source": [
    "Now we need something that could remember previous information as well as remembering info for a long period of time.\n",
    "HA! That's the classical BiDirectional RNN.\n",
    "Here I only implemented it with LSTM, but in practice it could be done with GRU or both interchangably.\n",
    "I will leave it to my partner to do some test running,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HH3pwb2_WXfg"
   },
   "source": [
    "Model Hyperparameters to test:\n",
    "1. [LSTM Hidden Nodes Number](#s31) \n",
    "2. [Density Layer](#s32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PST-XuCQCpL8"
   },
   "outputs": [],
   "source": [
    "model_BiD = Sequential()\n",
    "model_BiD.add(Embedding(vocab_size, emd_size, input_length=max_length,))\n",
    "model_BiD.add(LSTM(units=units,return_sequences = True))\n",
    "model_BiD.add(GlobalMaxPooling1D())\n",
    "model_BiD.add(Dense(hidden_dims,activation='relu'))\n",
    "model_BiD.add(Dropout(0.5))\n",
    "model_BiD.add(Dense(6,activation='sigmoid'))\n",
    "model_BiD.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model_BiD.summary()\n",
    "\n",
    "#Fit model\n",
    "history = model_BiD.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YjWCp3R6CpL-",
    "outputId": "46866dd5-754e-42c4-9d89-b17314774e71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159571/159571 [==============================] - 86s 539us/step\n",
      "Training accuracy: 0.98512\n",
      "63978/63978 [==============================] - 35s 553us/step\n",
      "Testing Accuracy: 0.97213\n"
     ]
    }
   ],
   "source": [
    "loss,accuracy = model_BiD.evaluate(x_train,y_train)\n",
    "print(\"Training accuracy: {:.5f}\".format(accuracy))\n",
    "loss,accuracy = model_BiD.evaluate(x_test,y_test)\n",
    "print(\"Testing Accuracy: {:.5f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "34jCHZyYlcWC"
   },
   "source": [
    "<a id=’s31’></a>\n",
    "### 3.1 LSTM Hidden Nodes Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cijIriQRm5ti"
   },
   "outputs": [],
   "source": [
    "def model_birnn_lstm(units):\n",
    "    inp = Input(shape=(max_length,))\n",
    "    x = Embedding(vocab_size, emd_size)(inp)\n",
    "    \n",
    "    x = Bidirectional(CuDNNLSTM(units=units, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    conc = Dense(hidden_dims, activation=\"relu\")(conc)\n",
    "    conc = Dropout(0.2)(conc)\n",
    "    outp = Dense(6, activation=\"sigmoid\")(conc)\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mvRFPZzmmH5M"
   },
   "source": [
    "case 3.1.1 units = 94 (alph = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uIhtqqTJ2XUA"
   },
   "outputs": [],
   "source": [
    "units = 94\n",
    "\n",
    "model_BiD2 = Sequential()\n",
    "model_BiD2.add(Embedding(vocab_size, emd_size, input_length=max_length,))\n",
    "model_BiD2.add(LSTM(units=units,return_sequences = True))\n",
    "model_BiD2.add(GlobalMaxPooling1D())2\n",
    "model_BiD2.add(Dense(hidden_dims,activation='relu'))\n",
    "model_BiD2.add(Dropout(0.2))\n",
    "model_BiD2.add(Dense(6,activation='sigmoid'))\n",
    "model_BiD2.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model_BiD2.summary()\n",
    "\n",
    "#Fit model\n",
    "history = model_BiD2.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GkwBQuEcnQjJ"
   },
   "outputs": [],
   "source": [
    "loss,accuracy = model_BiD2.evaluate(x_train,y_train)\n",
    "print(\"Training accuracy: {:.5f}\".format(accuracy))\n",
    "loss,accuracy = model_BiD2.evaluate(x_test,y_test)\n",
    "print(\"Testing Accuracy: {:.5f}\".format(accuracy))\n",
    "\n",
    "\"\"\" Visualize Training Results \"\"\"\n",
    "# Get training results\n",
    "history_dict = history.history\n",
    "train_acc = history_dict['accuracy']\n",
    "test_acc = history_dict['val_accuracy']\n",
    "\n",
    "# Plot training results\n",
    "plt.plot(train_acc, label='Train Acc.')\n",
    "plt.plot(test_acc, label='Test Acc.')\n",
    "\n",
    "# Show plot\n",
    "plt.title('Model Training Results(RNN: units=94)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UNIBYLMZndCl"
   },
   "source": [
    "case 3.1.2 units = 47 (alph = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BfCsALxpndMA"
   },
   "outputs": [],
   "source": [
    "units = 47\n",
    "model_BiD3 = Sequential()\n",
    "model_BiD3.add(Embedding(vocab_size, emd_size, input_length=max_length,))\n",
    "model_BiD3.add(LSTM(units=units,return_sequences = True))\n",
    "model_BiD3.add(GlobalMaxPooling1D())2\n",
    "model_BiD3.add(Dense(hidden_dims,activation='relu'))\n",
    "model_BiD3.add(Dropout(0.2))\n",
    "model_BiD3.add(Dense(6,activation='sigmoid'))\n",
    "model_BiD3.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model_BiD3.summary()\n",
    "\n",
    "#Fit model\n",
    "history = model_BiD3.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fIC648VPnr6v"
   },
   "outputs": [],
   "source": [
    "loss,accuracy = model_BiD3.evaluate(x_train,y_train)\n",
    "print(\"Training accuracy: {:.5f}\".format(accuracy))\n",
    "loss,accuracy = model_BiD3.evaluate(x_test,y_test)\n",
    "print(\"Testing Accuracy: {:.5f}\".format(accuracy))\n",
    "\n",
    "\"\"\" Visualize Training Results \"\"\"\n",
    "# Get training results\n",
    "history_dict = history.history\n",
    "train_acc = history_dict['accuracy']\n",
    "test_acc = history_dict['val_accuracy']\n",
    "\n",
    "# Plot training results\n",
    "plt.plot(train_acc, label='Train Acc.')\n",
    "plt.plot(test_acc, label='Test Acc.')\n",
    "\n",
    "# Show plot\n",
    "plt.title('Model Training Results(RNN: units=47)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e9yVFUVJnm7u"
   },
   "source": [
    "case 3.1.3 units = 31 (alph = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d5tS7QsqngTt"
   },
   "outputs": [],
   "source": [
    "units = 31\n",
    "model_BiD4 = Sequential()\n",
    "model_BiD4.add(Embedding(vocab_size, emd_size, input_length=max_length,))\n",
    "model_BiD4.add(LSTM(units=units,return_sequences = True))\n",
    "model_BiD4.add(GlobalMaxPooling1D())2\n",
    "model_BiD4.add(Dense(hidden_dims,activation='relu'))\n",
    "model_BiD4.add(Dropout(0.2))\n",
    "model_BiD4.add(Dense(6,activation='sigmoid'))\n",
    "model_BiD4.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model_BiD4.summary()\n",
    "\n",
    "#Fit model\n",
    "history = model_BiD4.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tqKHvZ_6nwSP"
   },
   "outputs": [],
   "source": [
    "loss,accuracy = model_BiD4.evaluate(x_train,y_train)\n",
    "print(\"Training accuracy: {:.5f}\".format(accuracy))\n",
    "loss,accuracy = model_BiD4.evaluate(x_test,y_test)\n",
    "print(\"Testing Accuracy: {:.5f}\".format(accuracy))\n",
    "\n",
    "\"\"\" Visualize Training Results \"\"\"\n",
    "# Get training results\n",
    "history_dict = history.history\n",
    "train_acc = history_dict['accuracy']\n",
    "test_acc = history_dict['val_accuracy']\n",
    "\n",
    "# Plot training results\n",
    "plt.plot(train_acc, label='Train Acc.')\n",
    "plt.plot(test_acc, label='Test Acc.')\n",
    "\n",
    "# Show plot\n",
    "plt.title('Model Training Results(RNN: units=31)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3xTQT3IBoCkx"
   },
   "source": [
    "<a id=’s32’></a>\n",
    "### 3.2 Density Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Id_XEWyOn09M"
   },
   "source": [
    "case 3.2.1: For case 3.1.1 delete middle density layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PSv4wvDCWUo6"
   },
   "outputs": [],
   "source": [
    "units = 94\n",
    "\n",
    "model_BiD5 = Sequential()\n",
    "model_BiD5.add(Embedding(vocab_size, emd_size, input_length=max_length,))\n",
    "model_BiD5.add(LSTM(units=units,return_sequences = True))\n",
    "model_BiD5.add(GlobalMaxPooling1D())2\n",
    "model_BiD5.add(Dropout(0.2))\n",
    "model_BiD5.add(Dense(6,activation='sigmoid'))\n",
    "model_BiD5.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model_BiD5.summary()\n",
    "\n",
    "#Fit model\n",
    "history = model_BiD5.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mDmBS64goXq8"
   },
   "outputs": [],
   "source": [
    "loss,accuracy = model_BiD5.evaluate(x_train,y_train)\n",
    "print(\"Training accuracy: {:.5f}\".format(accuracy))\n",
    "loss,accuracy = model_BiD5.evaluate(x_test,y_test)\n",
    "print(\"Testing Accuracy: {:.5f}\".format(accuracy))\n",
    "\n",
    "\"\"\" Visualize Training Results \"\"\"\n",
    "# Get training results\n",
    "history_dict = history.history\n",
    "train_acc = history_dict['accuracy']\n",
    "test_acc = history_dict['val_accuracy']\n",
    "\n",
    "# Plot training results\n",
    "plt.plot(train_acc, label='Train Acc.')\n",
    "plt.plot(test_acc, label='Test Acc.')\n",
    "\n",
    "# Show plot\n",
    "plt.title('Model Training Results(RNN: delete middle density layer)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xHo0yeaiCpMC"
   },
   "source": [
    "Till now, the test accuracy of various models is\n",
    "LSTM:          0.96839\n",
    "CNN:           0.97131\n",
    "CNN+ LSTM:     0.96890 \n",
    "BiDir RNN:     0.97213\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jlQ3-bPJCpMD"
   },
   "source": [
    "<a id=’p24’></a>\n",
    "## 4.Attention Models\n",
    "It's not covered in the lecture but since the release of Hierarchical Attention Networks for Document Classification paper written jointly by CMU and Microsoft guys in 2016, it's been quite popular.\n",
    "But what is the REAL incentive after trying this model?\n",
    "It's from the REAL TRUMP: \"what do you have to lose? I say, take it.\", so here I will give it a shoot as the president does to the hydroxychloroquine.\n",
    "\n",
    "As below is a simple attention model which help us by pay more attention to some word since toxic comments tend to be determined by just one or two toxic words, especially some 4 letter word, u know.\n",
    "\n",
    "Obviously, attention can be implemented together with models mentioned above, but since I don't have such time, I will leave it to my partner to do some trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 751,
     "status": "error",
     "timestamp": 1588548949195,
     "user": {
      "displayName": "Yuyang Lin",
      "photoUrl": "",
      "userId": "11037694261943317117"
     },
     "user_tz": -480
    },
    "id": "Ocgy7sGSCpMD",
    "outputId": "1110de4e-3551-42f7-aa95-9cfaaece1e92"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-f322ca21ee00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     def __init__(self, step_dim,\n\u001b[1;32m      3\u001b[0m                  \u001b[0mW_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                  \u001b[0mW_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                  bias=True, **kwargs):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Layer' is not defined"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/qqgeogor/keras-lstm-attention-glove840b-lb-0-043\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(name='{}_W'.format(self.name),\n",
    "                                 shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(name='{}_b'.format(self.name),\n",
    "                                     shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        e = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))  # e = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            e += self.b\n",
    "        e = K.tanh(e)\n",
    "\n",
    "        a = K.exp(e)\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "\n",
    "        c = K.sum(a * x, axis=1)\n",
    "        return c\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 900,
     "status": "error",
     "timestamp": 1588548942166,
     "user": {
      "displayName": "Yuyang Lin",
      "photoUrl": "",
      "userId": "11037694261943317117"
     },
     "user_tz": -480
    },
    "id": "9N55nlFGCpMF",
    "outputId": "3aa845ce-5560-4a06-c773-44a1529a85a9"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-d899a79550a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memd_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGlobalMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_dims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Attention' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "model_a = Sequential()\n",
    "model_a.add(Embedding(vocab_size, emd_size, input_length=max_length,))\n",
    "model_a.add(LSTM(units=units,return_sequences = True))\n",
    "model_a.add(Attention(200))\n",
    "model_a.add(GlobalMaxPooling1D())\n",
    "model_a.add(Dense(hidden_dims,activation='relu'))\n",
    "model_a.add(Dropout(0.5))\n",
    "model_a.add(Dense(6,activation='sigmoid'))\n",
    "model_a.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model_a.summary()\n",
    "\n",
    "#Fit model\n",
    "history = model_a.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MyektH4QCpMG"
   },
   "source": [
    "Also you can try different model layers, but to compare the result, I implemented the same layers.\n",
    "As below is another way to give it a shot, it's said to be 99% accuracy;\n",
    "https://www.kaggle.com/sanket30/cudnnlstm-lstm-99-accuracy\n",
    "You can dig in to find out why his model is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AKPx7uzICpMH"
   },
   "outputs": [],
   "source": [
    "loss,accuracy = model_a.evaluate(x_train,y_train)\n",
    "print(\"Training accuracy: {:.5f}\".format(accuracy))\n",
    "loss,accuracy = model_a.evaluate(x_test,y_test)\n",
    "print(\"Testing Accuracy: {:.5f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y3GUjjllbXiq"
   },
   "source": [
    "case 4.1: Add one more LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DSu5mfPsaq86"
   },
   "outputs": [],
   "source": [
    "model_a = Sequential()\n",
    "model_a.add(Embedding(vocab_size, emd_size, input_length=max_length,))\n",
    "model_a.add(LSTM(units=units,return_sequences = True))\n",
    "\n",
    "model_a.add(LSTM(units=units, return_sequences=True))\n",
    "model_a.add(LSTM(units=units, return_sequences=True))\n",
    "model_a.add(Attention())\n",
    "model_a.add(Dense(hidden_dims,activation='relu'))\n",
    "model_a.add(Dropout(0.2))\n",
    "model_a.add(Dense(6,activation='sigmoid'))\n",
    "model_a.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model_a.summary()\n",
    "\n",
    "#Fit model\n",
    "history = model_a.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4JB0XcwHCpMJ"
   },
   "source": [
    "## 5.BERT\n",
    "As mentioned in the comment, \"it's not difficult to implement BERT\",so BERT is not implemented. In addition, you may not want to download pretrained model which takes a while through streaming."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "comp3359proj.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
