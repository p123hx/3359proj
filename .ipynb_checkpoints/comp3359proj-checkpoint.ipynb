{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35TOzJw2CpLA"
   },
   "source": [
    "### COMP3359 Final Project: Toxic Comment Classification\n",
    "\n",
    "\n",
    "## Objective\n",
    "There are enormous discussions happening in our social media everyday and just one toxic comment can sour the whole discussion. Many platforms are struggling to effectively keep the environment clean. And how to define toxic may partly depend on what the platform is. Like some legal adult websites may be ok with obscene, but most are not. \n",
    "\n",
    "Therefore, the artificial intelligence program will not only detect the toxicity of the comment, but also label the comment with its toxicity type if it is. \n",
    "\n",
    "\n",
    "## Overview \n",
    "[Start With a Simple Nutrual Network](#p1) \n",
    "\n",
    "[Experiment With Other Models](#p2) \n",
    "\n",
    "[Test Performance of One Model](#p3)\n",
    "\n",
    " \n",
    "## Instruction\n",
    "\n",
    "The first(start with a simple nutrual network) and second(experiment with other models) parts are both about data preprocessing, building model and trainning model, and the third part is to see the performance of one model. \n",
    "\n",
    "__For example, if you want to test the simplest model in case0.1.1 (model1)__\n",
    "\n",
    "step1: run all the codes in `Before start`\n",
    "\n",
    "step2: run first two parts of the codes in `Start with a simple nutrual network` (`hyperparameters` and `data preprocess`)\n",
    "\n",
    "step3: run case0.1.1 code in third part(`build and train mulitple models`) of `Start with a simple nutrual network`\n",
    "\n",
    "step4: go to `test performace of one model`, change the model variable to the model you want to test, like model1, and run codes in 'test performance of one model'\n",
    "\n",
    "For all cases in `Start with a simple nutrual network`, step1 and step2 are the same, and run corresponding code in step3.\n",
    "\n",
    "__Another example, if you want to test the model in second part, like case 1.1.2 (model_cnn7, we found it has the best performance)__\n",
    "\n",
    "step1: run all the codes in `Before start` (same as before)\n",
    "\n",
    "step2: run first part codes in `Experiment with other models` (`More consistent variables for other models`)\n",
    "\n",
    "step3: run case 1.2.3 codes\n",
    "\n",
    "step4: go to `test performace of one model`, change the model variable to the model you want to test, like model_cnn7, and run codes in 'test performance of one model' (same as before)\n",
    "\n",
    "For all cases in `Experiment with other models`, step1 and step2 are the same, and run corresponding code in step3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this is optional (actually not necessary at all, who else will use google's laggy service)\n",
    "\n",
    "# \"\"\" Prepare Notebook for Google Colab \"\"\"\n",
    "# # Mount Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# # Specify directory of course materials in Google Drive\n",
    "# module_dir = '/content/drive/My Drive/3359proj/'\n",
    "\n",
    "# # Add course material directory in Google Drive to system path, for importing .py files later\n",
    "# # (Ref.: https://stackoverflow.com/questions/48905127/importing-py-files-in-google-colab)\n",
    "# import sys\n",
    "# sys.path.append(module_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare prerequist library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in ./anaconda3/lib/python3.7/site-packages (2.3.1)\n",
      "Requirement already satisfied: pyyaml in ./anaconda3/lib/python3.7/site-packages (from keras) (5.1.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in ./anaconda3/lib/python3.7/site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: six>=1.9.0 in ./anaconda3/lib/python3.7/site-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: scipy>=0.14 in ./anaconda3/lib/python3.7/site-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in ./anaconda3/lib/python3.7/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in ./anaconda3/lib/python3.7/site-packages (from keras) (1.17.2)\n",
      "Requirement already satisfied: h5py in ./anaconda3/lib/python3.7/site-packages (from keras) (2.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "!pip install tensorflow==1.13.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "epochs = 5\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Load Data \"\"\"\n",
    "\n",
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, Dropout, Flatten, MaxPooling1D, Concatenate, GlobalMaxPooling1D\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "# # this is for google colab( DONOT RUN IT if working locally)\n",
    "# data_dir = os.path.join(module_dir, \"input/\")\n",
    "# data_path_train = os.path.join(data_dir, \"train.csv\")\n",
    "# data_path_test = os.path.join(data_dir, \"test.csv\")\n",
    "# data_path_test_label = os.path.join(data_dir, \"test_labels.csv\")\n",
    "# train = pd.read_csv(data_path_train)\n",
    "# test = pd.read_csv(data_path_test)\n",
    "# test_label = pd.read_csv(data_path_test_label)\n",
    "\n",
    "train = pd.read_csv('./input/train.csv')\n",
    "test = pd.read_csv('./input/test.csv')\n",
    "test_label = pd.read_csv('./input/test_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(id               False\n",
       " comment_text     False\n",
       " toxic            False\n",
       " severe_toxic     False\n",
       " obscene          False\n",
       " threat           False\n",
       " insult           False\n",
       " identity_hate    False\n",
       " dtype: bool, id              False\n",
       " comment_text    False\n",
       " dtype: bool)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the data has the null input if so, do some data engineering\n",
    "train.isnull().any(),test.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[test_label['toxic'] != -1]\n",
    "test_label = test_label[test_label['toxic'] != -1]\n",
    "\n",
    "sen_train = train[\"comment_text\"]\n",
    "sen_test = test[\"comment_text\"]\n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y_train = train[list_classes].values\n",
    "y_test = test_label[list_classes].values\n",
    "\n",
    "\n",
    "# # Analyse train and test data set\n",
    "# unlabelled_in_train = train[(train['toxic']!=1) & (train['severe_toxic']!=1) & (train['obscene']!=1) & \n",
    "#                             (train['threat']!=1) & (train['insult']!=1) & (train['identity_hate']!=1)]\n",
    "# print('Percentage of unlabelled comments in train set is ', len(unlabelled_in_train)/len(train)*100)\n",
    "\n",
    "# unlabelled_in_test = test_label[(test_label['toxic']!=1) & (test_label['severe_toxic']!=1) & (test_label['obscene']!=1) & \n",
    "#                             (test_label['threat']!=1) & (test_label['insult']!=1) & (test_label['identity_hate']!=1)]\n",
    "# print('Percentage of unlabelled comments in test set is ', len(unlabelled_in_test)/len(test_label)*100)\n",
    "\n",
    "# print(train[list_classes].sum())\n",
    "# print(test_label[list_classes].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wfbr8Iuy9XHf"
   },
   "source": [
    "<a id='p1'></a>\n",
    "# Start With a Simple Nutrual Network\n",
    "\n",
    "This section implemented a model with embedding + 1-Max pooling layer + 1 dense layer.\n",
    "\n",
    "Model Hyperparameters to test:\n",
    "1. [Sentence Max Length: 100, 200, 400](#p11) \n",
    "2. [Word Embedding Dimension: 64, 128, 256](#p12) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as usually DON'T RUN it in order to save your time\n",
    "# visualize the length of sentences to help us choose max_length\n",
    "# totalNumWords = [len(one_comment) for one_comment in tokenized_train]\n",
    "# plt.hist(totalNumWords,bins = np.arange(0,410,10))#[0,50,100,150,200,250,300,350,400])#,450,500,550,600,650,700,750,800,850,900])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length200 = 200\n",
    "max_length400 = 400\n",
    "max_length100 = 100\n",
    "emd_size = 128\n",
    "emd_size64 = 64\n",
    "emd_size256 = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rCyKJy_ZpQwd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocab size:  210337\n",
      "Vocab in use:  20000\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary size for tokenization\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(list(sen_train))\n",
    "print(\"Total vocab size: \", len(list(tokenizer.word_index.items())))\n",
    "print(\"Vocab in use: \", tokenizer.num_words)\n",
    "tokenized_train = tokenizer.texts_to_sequences(sen_train)\n",
    "tokenized_test = tokenizer.texts_to_sequences(sen_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train Mulitple Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save time, run corresponding codes of the model you want to test in this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RMtO4ivXCRF5"
   },
   "source": [
    "<a id='p11'></a>\n",
    "### 1.  Sentence Max Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5GacajKRq-Dw"
   },
   "source": [
    "case 0.1.1: max length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3204,
     "status": "ok",
     "timestamp": 1588546623769,
     "user": {
      "displayName": "Yuyang Lin",
      "photoUrl": "",
      "userId": "11037694261943317117"
     },
     "user_tz": -480
    },
    "id": "BcDZsA3bpQwj",
    "outputId": "a28ef86a-b343-4326-a3f6-f17ffed755dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     (None, 200, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_14 (Glo (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 2,560,774\n",
      "Trainable params: 2,560,774\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x_train = pad_sequences(tokenized_train, maxlen=max_length200)\n",
    "x_test = pad_sequences(tokenized_test, maxlen=max_length200)\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(vocab_size, emd_size, input_length=max_length200,))\n",
    "model1.add(GlobalMaxPooling1D())\n",
    "model1.add(Dense(6,activation='sigmoid'))\n",
    "model1.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AuZWH4TUpQwm",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159571 samples, validate on 63978 samples\n",
      "Epoch 1/5\n",
      "159571/159571 [==============================] - 96s 602us/step - loss: 0.0921 - accuracy: 0.9730 - val_loss: 0.0755 - val_accuracy: 0.9716\n",
      "Epoch 2/5\n",
      "159571/159571 [==============================] - 95s 593us/step - loss: 0.0490 - accuracy: 0.9823 - val_loss: 0.0728 - val_accuracy: 0.9714\n",
      "Epoch 3/5\n",
      "159571/159571 [==============================] - 93s 584us/step - loss: 0.0433 - accuracy: 0.9839 - val_loss: 0.0723 - val_accuracy: 0.9710\n",
      "Epoch 4/5\n",
      "159571/159571 [==============================] - 95s 597us/step - loss: 0.0396 - accuracy: 0.9849 - val_loss: 0.0706 - val_accuracy: 0.9715\n",
      "Epoch 5/5\n",
      "159571/159571 [==============================] - 95s 593us/step - loss: 0.0367 - accuracy: 0.9860 - val_loss: 0.0726 - val_accuracy: 0.9710\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "history = model1.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nt7OryT5rEis"
   },
   "source": [
    "case 0.1.2: max length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qEnMqLuMHkzB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159571 samples, validate on 63978 samples\n",
      "Epoch 1/5\n",
      "159571/159571 [==============================] - 88s 550us/step - loss: 0.0940 - accuracy: 0.9727 - val_loss: 0.0780 - val_accuracy: 0.9711\n",
      "Epoch 2/5\n",
      "159571/159571 [==============================] - 86s 540us/step - loss: 0.0517 - accuracy: 0.9815 - val_loss: 0.0764 - val_accuracy: 0.9700\n",
      "Epoch 3/5\n",
      "159571/159571 [==============================] - 87s 546us/step - loss: 0.0458 - accuracy: 0.9831 - val_loss: 0.0708 - val_accuracy: 0.9719\n",
      "Epoch 4/5\n",
      "159571/159571 [==============================] - 89s 555us/step - loss: 0.0420 - accuracy: 0.9843 - val_loss: 0.0746 - val_accuracy: 0.9700\n",
      "Epoch 5/5\n",
      "159571/159571 [==============================] - 87s 546us/step - loss: 0.0390 - accuracy: 0.9851 - val_loss: 0.0736 - val_accuracy: 0.9704\n"
     ]
    }
   ],
   "source": [
    "x_train = pad_sequences(tokenized_train, maxlen=max_length100)\n",
    "x_test = pad_sequences(tokenized_test, maxlen=max_length100)\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(vocab_size, emd_size, input_length=max_length100,))\n",
    "model2.add(GlobalMaxPooling1D())\n",
    "model2.add(Dense(6,activation='sigmoid'))\n",
    "model2.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "history = model2.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UQFLPtQprM8z"
   },
   "source": [
    "case 0.1.3: max length = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rXsHXJjKjSlZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159571 samples, validate on 63978 samples\n",
      "Epoch 1/5\n",
      "159571/159571 [==============================] - 105s 659us/step - loss: 0.0917 - accuracy: 0.9735 - val_loss: 0.0797 - val_accuracy: 0.9702\n",
      "Epoch 2/5\n",
      "159571/159571 [==============================] - 104s 652us/step - loss: 0.0497 - accuracy: 0.9820 - val_loss: 0.0750 - val_accuracy: 0.9705\n",
      "Epoch 3/5\n",
      "159571/159571 [==============================] - 105s 655us/step - loss: 0.0442 - accuracy: 0.9836 - val_loss: 0.0722 - val_accuracy: 0.9709\n",
      "Epoch 4/5\n",
      "159571/159571 [==============================] - 105s 656us/step - loss: 0.0402 - accuracy: 0.9848 - val_loss: 0.0771 - val_accuracy: 0.9689\n",
      "Epoch 5/5\n",
      "159571/159571 [==============================] - 105s 656us/step - loss: 0.0372 - accuracy: 0.9857 - val_loss: 0.0733 - val_accuracy: 0.9702\n"
     ]
    }
   ],
   "source": [
    "x_train = pad_sequences(tokenized_train, maxlen=max_length400)\n",
    "x_test = pad_sequences(tokenized_test, maxlen=max_length400)\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(vocab_size, emd_size, input_length=max_length400,))\n",
    "model3.add(GlobalMaxPooling1D())\n",
    "model3.add(Dense(6,activation='sigmoid'))\n",
    "model3.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = model3.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When max_length = 200, val_accuracy = 0.9715, after 4 epochs.\n",
    "\n",
    "When max_length = 100, val_accuracy = 0.9719, after 3 epoch.\n",
    "\n",
    "When max_length = 400, val_accuracy = 0.9709, after 3 epochs.\n",
    "\n",
    "Therefore, we consider max_length = 100 as the optimal max_length, and apply it to later experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CT1DsuilCimA"
   },
   "source": [
    "<a id='p12'></a>\n",
    "### 2. Word Embedding Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "REYhfzkhrVs8"
   },
   "source": [
    "case 0.2.1: emd size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ui1DE_j602Ys"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159571 samples, validate on 63978 samples\n",
      "Epoch 1/5\n",
      "159571/159571 [==============================] - 58s 361us/step - loss: 0.1067 - accuracy: 0.9707 - val_loss: 0.0822 - val_accuracy: 0.9705\n",
      "Epoch 2/5\n",
      "159571/159571 [==============================] - 58s 362us/step - loss: 0.0561 - accuracy: 0.9803 - val_loss: 0.0795 - val_accuracy: 0.9695\n",
      "Epoch 3/5\n",
      "159571/159571 [==============================] - 57s 358us/step - loss: 0.0503 - accuracy: 0.9818 - val_loss: 0.0762 - val_accuracy: 0.9700\n",
      "Epoch 4/5\n",
      "159571/159571 [==============================] - 57s 360us/step - loss: 0.0464 - accuracy: 0.9829 - val_loss: 0.0739 - val_accuracy: 0.9705\n",
      "Epoch 5/5\n",
      "159571/159571 [==============================] - 57s 358us/step - loss: 0.0434 - accuracy: 0.9837 - val_loss: 0.0733 - val_accuracy: 0.9706\n"
     ]
    }
   ],
   "source": [
    "x_train = pad_sequences(tokenized_train, maxlen=max_length100)\n",
    "x_test = pad_sequences(tokenized_test, maxlen=max_length100)\n",
    "\n",
    "model4 = Sequential()\n",
    "model4.add(Embedding(vocab_size, emd_size64, input_length=max_length100,))\n",
    "model4.add(GlobalMaxPooling1D())\n",
    "model4.add(Dense(6,activation='sigmoid'))\n",
    "model4.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = model4.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CTVK3WhUrd8b"
   },
   "source": [
    "case 0.2.2: emd size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N4O3ye1X1qqt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159571 samples, validate on 63978 samples\n",
      "Epoch 1/5\n",
      "159571/159571 [==============================] - 167s 1ms/step - loss: 0.0838 - accuracy: 0.9747 - val_loss: 0.0748 - val_accuracy: 0.9713\n",
      "Epoch 2/5\n",
      "159571/159571 [==============================] - 164s 1ms/step - loss: 0.0474 - accuracy: 0.9827 - val_loss: 0.0681 - val_accuracy: 0.9730\n",
      "Epoch 3/5\n",
      "159571/159571 [==============================] - 165s 1ms/step - loss: 0.0413 - accuracy: 0.9845 - val_loss: 0.0733 - val_accuracy: 0.9706\n",
      "Epoch 4/5\n",
      "159571/159571 [==============================] - 166s 1ms/step - loss: 0.0372 - accuracy: 0.9857 - val_loss: 0.0732 - val_accuracy: 0.9705\n",
      "Epoch 5/5\n",
      "159571/159571 [==============================] - 164s 1ms/step - loss: 0.0337 - accuracy: 0.9871 - val_loss: 0.0712 - val_accuracy: 0.9717\n"
     ]
    }
   ],
   "source": [
    "x_train = pad_sequences(tokenized_train, maxlen=max_length100)\n",
    "x_test = pad_sequences(tokenized_test, maxlen=max_length100)\n",
    "\n",
    "model5 = Sequential()\n",
    "model5.add(Embedding(vocab_size, emd_size256, input_length=max_length100,))\n",
    "model5.add(GlobalMaxPooling1D())\n",
    "model5.add(Dense(6,activation='sigmoid'))\n",
    "model5.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = model5.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When emd_size = 64, the highest val_accuracy = 0.9706, after 5 epoch, as the loss and val_loss are still decreasing, it has the promise to get higher val_accuracy if there are more epochs.\n",
    "\n",
    "When emd_size = 256, the highest val_accuracy = 0.9730, after 2 epochs. It's much higher than 0.9706, although the former has promise, we would think emd_size = 256 has better performance.\n",
    "\n",
    "When emd_size = 128, the val_accuracy = 0.9719, after 3 epochs.\n",
    "\n",
    "Therefore, we choose emd_size = 256 for further tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "weyQGVAhCpLu"
   },
   "source": [
    "<br><br>\n",
    "\n",
    "<a id='p2'></a>\n",
    "# Experiment with other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eMcR8x5tsWIV"
   },
   "source": [
    "1. [TextCNN](#p21) \n",
    "2. [CNN+LSTM](#p22) \n",
    "3. [BiDirectional RNN(LSTM/GRU)](#p23)\n",
    "4. [Attention](#p24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Consistent variables for other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aGtcsnJ1pEHT"
   },
   "outputs": [],
   "source": [
    "#Preprocessing  is the same; Running the following when the padding is finished\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Conv1D, LSTM, GlobalMaxPooling1D, Dense, Dropout, Flatten, MaxPooling1D, Input, Concatenate,Conv2D, MaxPool2D, Reshape, CuDNNLSTM\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "\"\"\" common variables for all models\"\"\"\n",
    "\n",
    "emd_size = 256 # optimal get from previous nutrual network\n",
    "max_length = 100 # optiaml get from previous nutrual network\n",
    "hidden_dims = 64\n",
    "\n",
    "x_train = pad_sequences(tokenized_train, maxlen=max_length)\n",
    "x_test = pad_sequences(tokenized_test, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oS3D423-CpLu"
   },
   "source": [
    "<a id='p21'></a>\n",
    "## 1.TextCNN\n",
    "According to the paper Convolutional Neural Networks for Sentence Classification by Yoon Kim, we can use word vector and use CNN as we do to images. The paper says CNN has excellent performance on sentence-level classification tasks with multiple benchmarks. So let's first give it a shoot and we will do some analysis once we have got the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BFZzGy2QWfy9"
   },
   "source": [
    "Model Hyperparameters to test:\n",
    "\n",
    "1. [Filter Size](#s11) \n",
    "2. [Density Layer](#s12)\n",
    "3. [Filter number](#s13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "case 1.0: filter size = 3, has internal density layer, filter number = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vlJrsnqeCpLw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_23 (Embedding)     (None, 100, 256)          5120000   \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 98, 250)           192250    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_21 (Glo (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 64)                16064     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 5,328,704\n",
      "Trainable params: 5,328,704\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhome/cs/linyy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159571 samples, validate on 63978 samples\n",
      "Epoch 1/5\n",
      "159571/159571 [==============================] - 251s 2ms/step - loss: 0.0665 - accuracy: 0.9779 - val_loss: 0.0723 - val_accuracy: 0.9712\n",
      "Epoch 2/5\n",
      "159571/159571 [==============================] - 252s 2ms/step - loss: 0.0478 - accuracy: 0.9822 - val_loss: 0.0725 - val_accuracy: 0.9718\n",
      "Epoch 3/5\n",
      "159571/159571 [==============================] - 251s 2ms/step - loss: 0.0401 - accuracy: 0.9843 - val_loss: 0.0777 - val_accuracy: 0.9686\n",
      "Epoch 4/5\n",
      "159571/159571 [==============================] - 249s 2ms/step - loss: 0.0337 - accuracy: 0.9861 - val_loss: 0.0949 - val_accuracy: 0.9643\n",
      "Epoch 5/5\n",
      "159571/159571 [==============================] - 250s 2ms/step - loss: 0.0296 - accuracy: 0.9878 - val_loss: 0.0930 - val_accuracy: 0.9684\n"
     ]
    }
   ],
   "source": [
    "#CNN model\n",
    "#try to choose the same parameter as before so that we can compare the result with regards to the model choice\n",
    "#Parameters without specific comment are subject to fine-tuning.\n",
    "# the reason why I set them to numerical value is that I don't want to re-run to load the parameter value specified before\n",
    "filters = 250\n",
    "kernal_size = 3 # normally is set to 3\n",
    "\n",
    "#but I don't want to waste time waiting so I will leave the training with larger epoch to my parter\n",
    "\n",
    "\n",
    "#As below is a shallow CNN; Definitely we can try with another deep CNN but it will take some time to fine-tuning as well hence\n",
    "# I will take care of that if I do have some spare time.\n",
    "# The reset is just routine with nothing worth mentioning.\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Embedding(vocab_size, emd_size, input_length=max_length,))\n",
    "model_cnn.add(Conv1D(filters, kernal_size, activation='relu'))\n",
    "model_cnn.add(GlobalMaxPooling1D())\n",
    "model_cnn.add(Dense(hidden_dims,activation='relu'))\n",
    "model_cnn.add(Dropout(0.5))\n",
    "model_cnn.add(Dense(6,activation='sigmoid'))\n",
    "model_cnn.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model_cnn.summary()\n",
    "\n",
    "#Fit model\n",
    "history = model_cnn.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gT__hC4yiGWC"
   },
   "source": [
    "<a id='s11'></a>\n",
    "### 1.1 Filter Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L4vhgMHIjOWi"
   },
   "source": [
    "case 1.1.1: Filter Size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iPAgbriaXy1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_29 (Embedding)     (None, 100, 256)          5120000   \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 93, 250)           512250    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_27 (Glo (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 64)                16064     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 5,648,704\n",
      "Trainable params: 5,648,704\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhome/cs/linyy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159571 samples, validate on 63978 samples\n",
      "Epoch 1/5\n",
      "159571/159571 [==============================] - 385s 2ms/step - loss: 0.0661 - accuracy: 0.9781 - val_loss: 0.0767 - val_accuracy: 0.9693\n",
      "Epoch 2/5\n",
      "159571/159571 [==============================] - 386s 2ms/step - loss: 0.0472 - accuracy: 0.9823 - val_loss: 0.0714 - val_accuracy: 0.9728\n",
      "Epoch 3/5\n",
      "159571/159571 [==============================] - 382s 2ms/step - loss: 0.0388 - accuracy: 0.9845 - val_loss: 0.0794 - val_accuracy: 0.9679\n",
      "Epoch 4/5\n",
      "159571/159571 [==============================] - 395s 2ms/step - loss: 0.0327 - accuracy: 0.9866 - val_loss: 0.0815 - val_accuracy: 0.9693\n",
      "Epoch 5/5\n",
      "159571/159571 [==============================] - 423s 3ms/step - loss: 0.0287 - accuracy: 0.9880 - val_loss: 0.0980 - val_accuracy: 0.9686\n"
     ]
    }
   ],
   "source": [
    "filters = 250\n",
    "\n",
    "filter_size = 8\n",
    "\n",
    "model_cnn2 = Sequential()\n",
    "model_cnn2.add(Embedding(vocab_size, emd_size, input_length=max_length,))\n",
    "model_cnn2.add(Conv1D(filters, filter_size, activation='relu'))\n",
    "model_cnn2.add(GlobalMaxPooling1D())\n",
    "model_cnn2.add(Dense(hidden_dims,activation='relu'))\n",
    "model_cnn2.add(Dropout(0.5))\n",
    "model_cnn2.add(Dense(6,activation='sigmoid'))\n",
    "model_cnn2.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "print(model_cnn2.summary())\n",
    "\n",
    "#Fit model\n",
    "history = model_cnn2.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 1.0: filter size = 3, highest val_acc = 0.9718, after 2 epochs\n",
    "\n",
    "Case 1.1.1, filter size = 8, highest val_acc = 0.9728, after 2 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QhddnzFfjSBR"
   },
   "source": [
    "case 1.1.2: Filter Size = [3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "33c48M-YSNbK"
   },
   "outputs": [],
   "source": [
    "filters = 250\n",
    "\n",
    "filter_sizes = [3, 4, 5]\n",
    "\n",
    "model_cnn3 = Sequential()\n",
    "model_cnn3.add(Embedding(vocab_size, emd_size, input_length=max_length,))\n",
    "for i in range(len(filter_sizes)):\n",
    "  model_cnn3.add(Conv1D(filters, filter_sizes[i], activation='relu'))\n",
    "\n",
    "model_cnn3.add(GlobalMaxPooling1D())\n",
    "model_cnn3.add(Dense(hidden_dims,activation='relu'))\n",
    "model_cnn3.add(Dropout(0.5))\n",
    "model_cnn3.add(Dense(6,activation='sigmoid'))\n",
    "model_cnn3.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "print(model_cnn3.summary())\n",
    "\n",
    "#Fit model\n",
    "history = model_cnn3.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though filter size = 3 has worse performance than filter size = 8, when we use multiple filter size: [3,4,5], we get better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gm5m0Q46Xexf"
   },
   "source": [
    "<a id='s12'></a>\n",
    "### 1.2 Density Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wwryvn40kW7u"
   },
   "source": [
    "case 1.2.1: For case 1.0 remove one density layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_QWCdf_tVTra"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_31 (Embedding)     (None, 100, 256)          5120000   \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 98, 250)           192250    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_29 (Glo (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 6)                 1506      \n",
      "=================================================================\n",
      "Total params: 5,313,756\n",
      "Trainable params: 5,313,756\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhome/cs/linyy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159571 samples, validate on 63978 samples\n",
      "Epoch 1/5\n",
      "159571/159571 [==============================] - 260s 2ms/step - loss: 0.0657 - accuracy: 0.9782 - val_loss: 0.0695 - val_accuracy: 0.9727\n",
      "Epoch 2/5\n",
      "159571/159571 [==============================] - 258s 2ms/step - loss: 0.0488 - accuracy: 0.9825 - val_loss: 0.0736 - val_accuracy: 0.9712\n",
      "Epoch 3/5\n",
      "159571/159571 [==============================] - 257s 2ms/step - loss: 0.0432 - accuracy: 0.9841 - val_loss: 0.0747 - val_accuracy: 0.9717\n",
      "Epoch 4/5\n",
      "159571/159571 [==============================] - 257s 2ms/step - loss: 0.0389 - accuracy: 0.9856 - val_loss: 0.0792 - val_accuracy: 0.9718\n",
      "Epoch 5/5\n",
      "159571/159571 [==============================] - 254s 2ms/step - loss: 0.0347 - accuracy: 0.9871 - val_loss: 0.0825 - val_accuracy: 0.9723\n"
     ]
    }
   ],
   "source": [
    "filters = 250\n",
    "\n",
    "filter_size = 3\n",
    "\n",
    "model_cnn4 = Sequential()\n",
    "model_cnn4.add(Embedding(vocab_size, emd_size, input_length=max_length,))\n",
    "model_cnn4.add(Conv1D(filters, filter_size, activation='relu'))\n",
    "model_cnn4.add(GlobalMaxPooling1D())\n",
    "model_cnn4.add(Dropout(0.5))\n",
    "model_cnn4.add(Dense(6,activation='sigmoid'))\n",
    "model_cnn4.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "print(model_cnn4.summary())\n",
    "\n",
    "#Fit model\n",
    "history = model_cnn4.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "case 1.2.2: For case 1.1.1 remove one density layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_62\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_51 (Embedding)     (None, 100, 256)          5120000   \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 93, 250)           512250    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_39 (Glo (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 6)                 1506      \n",
      "=================================================================\n",
      "Total params: 5,633,756\n",
      "Trainable params: 5,633,756\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhome/cs/linyy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159571 samples, validate on 63978 samples\n",
      "Epoch 1/5\n",
      "159571/159571 [==============================] - 354s 2ms/step - loss: 0.0651 - accuracy: 0.9787 - val_loss: 0.0713 - val_accuracy: 0.9724\n",
      "Epoch 2/5\n",
      "159571/159571 [==============================] - 353s 2ms/step - loss: 0.0479 - accuracy: 0.9829 - val_loss: 0.0751 - val_accuracy: 0.9724\n",
      "Epoch 3/5\n",
      "159571/159571 [==============================] - 351s 2ms/step - loss: 0.0404 - accuracy: 0.9853 - val_loss: 0.0839 - val_accuracy: 0.9717\n",
      "Epoch 4/5\n",
      "159571/159571 [==============================] - 350s 2ms/step - loss: 0.0343 - accuracy: 0.9875 - val_loss: 0.0950 - val_accuracy: 0.9710\n",
      "Epoch 5/5\n",
      "159571/159571 [==============================] - 348s 2ms/step - loss: 0.0295 - accuracy: 0.9894 - val_loss: 0.1027 - val_accuracy: 0.9700\n"
     ]
    }
   ],
   "source": [
    "filters = 250\n",
    "\n",
    "filter_size = 8\n",
    "\n",
    "model_cnn8 = Sequential()\n",
    "model_cnn8.add(Embedding(vocab_size, emd_size, input_length=max_length,))\n",
    "model_cnn8.add(Conv1D(filters, filter_size, activation='relu'))\n",
    "model_cnn8.add(GlobalMaxPooling1D())\n",
    "model_cnn8.add(Dropout(0.5))\n",
    "model_cnn8.add(Dense(6,activation='sigmoid'))\n",
    "model_cnn8.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "print(model_cnn8.summary())\n",
    "\n",
    "#Fit model\n",
    "history = model_cnn8.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "case 1.2.3: For case 1.1.2 remove one density layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_63\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_52 (Embedding)     (None, 100, 256)          5120000   \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 98, 250)           192250    \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 95, 250)           250250    \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 91, 250)           312750    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_40 (Glo (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 6)                 1506      \n",
      "=================================================================\n",
      "Total params: 5,876,756\n",
      "Trainable params: 5,876,756\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhome/cs/linyy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159571 samples, validate on 63978 samples\n",
      "Epoch 1/5\n",
      "159571/159571 [==============================] - 452s 3ms/step - loss: 0.0721 - accuracy: 0.9770 - val_loss: 0.0731 - val_accuracy: 0.9727\n",
      "Epoch 2/5\n",
      "159571/159571 [==============================] - 449s 3ms/step - loss: 0.0536 - accuracy: 0.9811 - val_loss: 0.0816 - val_accuracy: 0.9693\n",
      "Epoch 3/5\n",
      "159571/159571 [==============================] - 452s 3ms/step - loss: 0.0476 - accuracy: 0.9830 - val_loss: 0.0948 - val_accuracy: 0.9664\n",
      "Epoch 4/5\n",
      "159571/159571 [==============================] - 445s 3ms/step - loss: 0.0426 - accuracy: 0.9845 - val_loss: 0.0781 - val_accuracy: 0.9724\n",
      "Epoch 5/5\n",
      "159571/159571 [==============================] - 442s 3ms/step - loss: 0.0384 - accuracy: 0.9860 - val_loss: 0.0845 - val_accuracy: 0.9718\n"
     ]
    }
   ],
   "source": [
    "filters = 250\n",
    "\n",
    "filter_size = [3,4,5]\n",
    "\n",
    "model_cnn7 = Sequential()\n",
    "model_cnn7.add(Embedding(vocab_size, emd_size, input_length=max_length,))\n",
    "for i in range(len(filter_sizes)):\n",
    "  model_cnn7.add(Conv1D(filters, filter_sizes[i], activation='relu'))\n",
    "\n",
    "model_cnn7.add(GlobalMaxPooling1D())\n",
    "model_cnn7.add(Dropout(0.5))\n",
    "model_cnn7.add(Dense(6,activation='sigmoid'))\n",
    "model_cnn7.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "print(model_cnn7.summary())\n",
    "\n",
    "#Fit model\n",
    "history = model_cnn7.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 1,2.1, remove one density layer from case 1.0. highest val_acc = 0.9727 after 1 epoch. But in case 1.0, with density layer, the highest val_acc = 0.9718, after 2 epochs. higher.\n",
    "\n",
    "Case 1.2.2, remove one density layer from case 1.1.1. Val_acc = 0.9724, after 1 epoch. And in case 1.1.1, val_acc = 0.9728, lower.\n",
    "\n",
    "Case 1.2.3, remove one density layer from case 1.1.2. highest val_acc = 0.9727, case 1.1.2, val_acc = 0.9739.\n",
    "\n",
    "Density layer is used to reshape the output. If itâ€™s in the middle, we think itâ€™s to get the features from higher dimensions to just 1D array, a.k.a extract the features. It seems that the density layer wonâ€™t help to enhance the performance when filter size is small. But when the filter size is big or complex, the density layer is helpful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='s13'></a>\n",
    "### 1.3 Filter Number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "case 1.3.1: filter number = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_32 (Embedding)     (None, 100, 256)          5120000   \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 98, 100)           76900     \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 95, 100)           40100     \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 91, 100)           50100     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_30 (Glo (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 64)                6464      \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 5,293,954\n",
      "Trainable params: 5,293,954\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhome/cs/linyy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159571 samples, validate on 63978 samples\n",
      "Epoch 1/5\n",
      "159571/159571 [==============================] - 280s 2ms/step - loss: 0.0713 - accuracy: 0.9773 - val_loss: 0.0824 - val_accuracy: 0.9680\n",
      "Epoch 2/5\n",
      "159571/159571 [==============================] - 276s 2ms/step - loss: 0.0512 - accuracy: 0.9816 - val_loss: 0.0841 - val_accuracy: 0.9682\n",
      "Epoch 3/5\n",
      "159571/159571 [==============================] - 276s 2ms/step - loss: 0.0456 - accuracy: 0.9828 - val_loss: 0.0917 - val_accuracy: 0.9662\n",
      "Epoch 4/5\n",
      "159571/159571 [==============================] - 277s 2ms/step - loss: 0.0414 - accuracy: 0.9840 - val_loss: 0.0884 - val_accuracy: 0.9665\n",
      "Epoch 5/5\n",
      "159571/159571 [==============================] - 279s 2ms/step - loss: 0.0376 - accuracy: 0.9853 - val_loss: 0.0890 - val_accuracy: 0.9673\n"
     ]
    }
   ],
   "source": [
    "filter_sizes = [3,4,5]\n",
    "filters = 100\n",
    "\n",
    "\n",
    "model_cnn5 = Sequential()\n",
    "model_cnn5.add(Embedding(vocab_size, emd_size, input_length=max_length,))\n",
    "for i in range(len(filter_sizes)):\n",
    "  model_cnn5.add(Conv1D(filters, filter_sizes[i], activation='relu'))\n",
    "\n",
    "model_cnn5.add(GlobalMaxPooling1D())\n",
    "model_cnn5.add(Dense(hidden_dims,activation='relu'))\n",
    "model_cnn5.add(Dropout(0.5))\n",
    "model_cnn5.add(Dense(6,activation='sigmoid'))\n",
    "model_cnn5.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "print(model_cnn5.summary())\n",
    "\n",
    "#Fit model\n",
    "history = model_cnn5.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "case 1.3.2: filter number = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_33 (Embedding)     (None, 100, 256)          5120000   \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 98, 500)           384500    \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 95, 500)           1000500   \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 91, 500)           1250500   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_31 (Glo (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 64)                32064     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 7,787,954\n",
      "Trainable params: 7,787,954\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhome/cs/linyy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159571 samples, validate on 63978 samples\n",
      "Epoch 1/5\n",
      "159571/159571 [==============================] - 1570s 10ms/step - loss: 0.0742 - accuracy: 0.9759 - val_loss: 0.0711 - val_accuracy: 0.9731\n",
      "Epoch 2/5\n",
      "159571/159571 [==============================] - 1573s 10ms/step - loss: 0.0539 - accuracy: 0.9808 - val_loss: 0.0939 - val_accuracy: 0.9648\n",
      "Epoch 3/5\n",
      "159571/159571 [==============================] - 1562s 10ms/step - loss: 0.0488 - accuracy: 0.9820 - val_loss: 0.0740 - val_accuracy: 0.9730\n",
      "Epoch 4/5\n",
      "159571/159571 [==============================] - 1551s 10ms/step - loss: 0.0446 - accuracy: 0.9830 - val_loss: 0.0907 - val_accuracy: 0.9673\n",
      "Epoch 5/5\n",
      "159571/159571 [==============================] - 1476s 9ms/step - loss: 0.0408 - accuracy: 0.9840 - val_loss: 0.0826 - val_accuracy: 0.9688\n"
     ]
    }
   ],
   "source": [
    "filter_sizes = [3,4,5]\n",
    "filters = 500\n",
    "\n",
    "\n",
    "model_cnn6 = Sequential()\n",
    "model_cnn6.add(Embedding(vocab_size, emd_size, input_length=max_length,))\n",
    "for i in range(len(filter_sizes)):\n",
    "  model_cnn6.add(Conv1D(filters, filter_sizes[i], activation='relu'))\n",
    "\n",
    "model_cnn6.add(GlobalMaxPooling1D())\n",
    "model_cnn6.add(Dense(hidden_dims,activation='relu'))\n",
    "model_cnn6.add(Dropout(0.5))\n",
    "model_cnn6.add(Dense(6,activation='sigmoid'))\n",
    "model_cnn6.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "print(model_cnn6.summary())\n",
    "\n",
    "#Fit model\n",
    "history = model_cnn6.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 1.3.1, filter number = 100, highest val_acc = 0.9682 after 2 epochs, val_loss = 0.0841\n",
    "\n",
    "Case 1.3.2, filter number = 500, highest val_acc = 0.9731 after epochs, val_losst = 0.0711\n",
    "\n",
    "Case 1.1.2, filter number = 250, highest val_acc = 0.9739 after 3 epochs, val_loss = 0.0716\n",
    "\n",
    "Larger filter number means for a same filter size window has more filters, which means learn more from the same filter window. But too many filters will make the train speed very slow, and might cause overfitting, like case 1.3.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best structure for textCNN in our case is that, kernel size = [3,4,5], filter size =250, with a density layer. The best val_acc = 0.9739 is in case 1.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4jcPsJMbCpL0"
   },
   "source": [
    "<a id='p22'></a>\n",
    "## 2.CNN + LSTM\n",
    "We have tried CNN above and will try LSTM below, now let's implement them together to see the result. The motivation is that I want to see how it handles long sequences together with what to keep and what to forget.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "case 2.0: Combine TextCNN and LSTM after getting the result of part 1 and 3\n",
    "\n",
    "combin case 1.1.2 and case 3.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Esdm7e6_CpL2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_61\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_50 (Embedding)     (None, 100, 256)          5120000   \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 98, 250)           192250    \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 95, 250)           250250    \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 91, 250)           312750    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 45, 250)           0         \n",
      "=================================================================\n",
      "Total params: 5,875,250\n",
      "Trainable params: 5,875,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_61\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_50 (Embedding)     (None, 100, 256)          5120000   \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 98, 250)           192250    \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 95, 250)           250250    \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 91, 250)           312750    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 45, 250)           0         \n",
      "_________________________________________________________________\n",
      "lstm_33 (LSTM)               (None, 50)                60200     \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 5,935,756\n",
      "Trainable params: 5,935,756\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhome/cs/linyy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159571 samples, validate on 63978 samples\n",
      "Epoch 1/5\n",
      "159571/159571 [==============================] - 691s 4ms/step - loss: 0.0777 - accuracy: 0.9754 - val_loss: 0.0950 - val_accuracy: 0.9619\n",
      "Epoch 2/5\n",
      "159571/159571 [==============================] - 681s 4ms/step - loss: 0.0547 - accuracy: 0.9805 - val_loss: 0.0838 - val_accuracy: 0.9691\n",
      "Epoch 3/5\n",
      "159571/159571 [==============================] - 682s 4ms/step - loss: 0.0483 - accuracy: 0.9823 - val_loss: 0.0817 - val_accuracy: 0.9703\n",
      "Epoch 4/5\n",
      "159571/159571 [==============================] - 670s 4ms/step - loss: 0.0427 - accuracy: 0.9839 - val_loss: 0.0956 - val_accuracy: 0.9649\n",
      "Epoch 5/5\n",
      "159571/159571 [==============================] - 668s 4ms/step - loss: 0.0385 - accuracy: 0.9853 - val_loss: 0.0903 - val_accuracy: 0.9703\n"
     ]
    }
   ],
   "source": [
    "#CNN model\n",
    "#try to choose the same parameter as before so that we can compare the result with regards to the model choice\n",
    "#Parameters without specific comment are subject to fine-tuning.\n",
    "# the reason why I set them to numerical value is that I don't want to re-run to load the parameter value specified before\n",
    "filters = 250 # optimal from previous tests\n",
    "kernal_size = [3,4,5] # optimal from previous tests\n",
    "units=50 # optimal from below tests\n",
    "#but I don't want to waste time waiting so I will leave the training with larger epoch to my parter\n",
    "\n",
    "\n",
    "#As below is a shallow CNN; Definitely we can try with another deep CNN but it will take some time to fine-tuning as well hence\n",
    "# I will take care of that if I do have some spare time.\n",
    "# The reset is just routine with nothing worth mentioning.\n",
    "model_cnn_lstm = Sequential()\n",
    "model_cnn_lstm.add(Embedding(vocab_size, emd_size, input_length=max_length,))\n",
    "for i in range(len(filter_sizes)):\n",
    "  model_cnn_lstm.add(Conv1D(filters, kernal_size[i], activation='relu'))\n",
    "model_cnn_lstm.add(MaxPooling1D()) # also try 2D \n",
    "model_cnn_lstm.summary()\n",
    "model_cnn_lstm.add(LSTM(units=units)) #same as the above\n",
    "model_cnn_lstm.add(Dropout(0.2))\n",
    "model_cnn_lstm.add(Dense(6,activation='sigmoid'))\n",
    "model_cnn_lstm.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model_cnn_lstm.summary()\n",
    "\n",
    "#Fit model\n",
    "history = model_cnn_lstm.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest val_acc = 0.9703 after 3 epochs. \n",
    "\n",
    "TextCNN has 0.9739 val_acc, and LSTM has 0.9737. The result shows that the combination makes no improvement.\n",
    "\n",
    "Possible reason for no improvement: We did the combination by adding an LSTM layer after CNN. That might not be a real combination, like when we do computation in CNN layers, it canâ€™t remember the previous thing until touching the LSTM layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zFoWJFpzCpL7"
   },
   "source": [
    "<a id='p23'></a>\n",
    "## 3.BiDirectional RNN(LSTM/GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DnXBYUcRCpL8"
   },
   "source": [
    "Now we need something that could remember previous information as well as remembering info for a long period of time.\n",
    "HA! That's the classical BiDirectional RNN.\n",
    "Here I only implemented it with LSTM, but in practice it could be done with GRU or both interchangably.\n",
    "I will leave it to my partner to do some test running,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HH3pwb2_WXfg"
   },
   "source": [
    "Model Hyperparameters to test:\n",
    "1. [LSTM Hidden Nodes Number](#s31) \n",
    "2. [Density Layer](#s32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "case 3.0: units = 50 (alph = 9), has internal density layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PST-XuCQCpL8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_35 (Embedding)     (None, 100, 256)          5120000   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100, 50)           61400     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_33 (Glo (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 64)                3264      \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 5,185,054\n",
      "Trainable params: 5,185,054\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhome/cs/linyy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159571 samples, validate on 63978 samples\n",
      "Epoch 1/5\n",
      "159571/159571 [==============================] - 678s 4ms/step - loss: 0.0754 - accuracy: 0.9761 - val_loss: 0.0702 - val_accuracy: 0.9725\n",
      "Epoch 2/5\n",
      "159571/159571 [==============================] - 674s 4ms/step - loss: 0.0482 - accuracy: 0.9825 - val_loss: 0.0743 - val_accuracy: 0.9705\n",
      "Epoch 3/5\n",
      "159571/159571 [==============================] - 667s 4ms/step - loss: 0.0423 - accuracy: 0.9838 - val_loss: 0.0726 - val_accuracy: 0.9707\n",
      "Epoch 4/5\n",
      "159571/159571 [==============================] - 666s 4ms/step - loss: 0.0372 - accuracy: 0.9854 - val_loss: 0.0809 - val_accuracy: 0.9679\n",
      "Epoch 5/5\n",
      "159571/159571 [==============================] - 676s 4ms/step - loss: 0.0329 - accuracy: 0.9867 - val_loss: 0.0809 - val_accuracy: 0.9696\n"
     ]
    }
   ],
   "source": [
    "units = 50\n",
    "\n",
    "model_BiD = Sequential()\n",
    "model_BiD.add(Embedding(vocab_size, emd_size, input_length=max_length,))\n",
    "model_BiD.add(LSTM(units=units,return_sequences = True))\n",
    "model_BiD.add(GlobalMaxPooling1D())\n",
    "model_BiD.add(Dense(hidden_dims,activation='relu'))\n",
    "model_BiD.add(Dropout(0.5))\n",
    "model_BiD.add(Dense(6,activation='sigmoid'))\n",
    "model_BiD.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model_BiD.summary()\n",
    "\n",
    "#Fit model\n",
    "history = model_BiD.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "34jCHZyYlcWC"
   },
   "source": [
    "<a id='s31'></a>\n",
    "### 3.1 LSTM Hidden Nodes Number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mvRFPZzmmH5M"
   },
   "source": [
    "case 3.1.1 units = 149 (alph = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uIhtqqTJ2XUA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_36 (Embedding)     (None, 100, 256)          5120000   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100, 149)          241976    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_34 (Glo (None, 149)               0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 64)                9600      \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 5,371,966\n",
      "Trainable params: 5,371,966\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhome/cs/linyy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159571 samples, validate on 63978 samples\n",
      "Epoch 1/5\n",
      "159571/159571 [==============================] - 1198s 8ms/step - loss: 0.0712 - accuracy: 0.9771 - val_loss: 0.0729 - val_accuracy: 0.9703\n",
      "Epoch 2/5\n",
      "159571/159571 [==============================] - 1185s 7ms/step - loss: 0.0464 - accuracy: 0.9828 - val_loss: 0.0684 - val_accuracy: 0.9721\n",
      "Epoch 3/5\n",
      "159571/159571 [==============================] - 1164s 7ms/step - loss: 0.0402 - accuracy: 0.9845 - val_loss: 0.0713 - val_accuracy: 0.9715\n",
      "Epoch 4/5\n",
      "159571/159571 [==============================] - 1163s 7ms/step - loss: 0.0352 - accuracy: 0.9861 - val_loss: 0.0712 - val_accuracy: 0.9712\n",
      "Epoch 5/5\n",
      "159571/159571 [==============================] - 1165s 7ms/step - loss: 0.0304 - accuracy: 0.9878 - val_loss: 0.0815 - val_accuracy: 0.9683\n"
     ]
    }
   ],
   "source": [
    "units = 149\n",
    "\n",
    "model_BiD2 = Sequential()\n",
    "model_BiD2.add(Embedding(vocab_size, emd_size, input_length=max_length,))\n",
    "model_BiD2.add(LSTM(units=units,return_sequences = True))\n",
    "model_BiD2.add(GlobalMaxPooling1D())\n",
    "model_BiD2.add(Dense(hidden_dims,activation='relu'))\n",
    "model_BiD2.add(Dropout(0.2))\n",
    "model_BiD2.add(Dense(6,activation='sigmoid'))\n",
    "model_BiD2.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model_BiD2.summary()\n",
    "\n",
    "#Fit model\n",
    "history = model_BiD2.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UNIBYLMZndCl"
   },
   "source": [
    "case 3.1.2 units = 74 (alph = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BfCsALxpndMA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 256)          5120000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100, 74)           97976     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4800      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 5,223,166\n",
      "Trainable params: 5,223,166\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /userhome/cs/linyy/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /userhome/cs/linyy/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 159571 samples, validate on 63978 samples\n",
      "Epoch 1/5\n",
      "159571/159571 [==============================] - 558s 3ms/step - loss: 0.0708 - accuracy: 0.9775 - val_loss: 0.0752 - val_accuracy: 0.9706\n",
      "Epoch 2/5\n",
      "159571/159571 [==============================] - 538s 3ms/step - loss: 0.0452 - accuracy: 0.9834 - val_loss: 0.0701 - val_accuracy: 0.9720\n",
      "Epoch 3/5\n",
      "159571/159571 [==============================] - 534s 3ms/step - loss: 0.0390 - accuracy: 0.9849 - val_loss: 0.0715 - val_accuracy: 0.9709\n",
      "Epoch 4/5\n",
      "159571/159571 [==============================] - 532s 3ms/step - loss: 0.0334 - accuracy: 0.9869 - val_loss: 0.0740 - val_accuracy: 0.9708\n",
      "Epoch 5/5\n",
      "159571/159571 [==============================] - 529s 3ms/step - loss: 0.0284 - accuracy: 0.9888 - val_loss: 0.0821 - val_accuracy: 0.9677\n"
     ]
    }
   ],
   "source": [
    "units = 74\n",
    "hidden_dims = 64\n",
    "\n",
    "model_BiD3 = Sequential()\n",
    "model_BiD3.add(Embedding(vocab_size, emd_size, input_length=max_length,))\n",
    "model_BiD3.add(LSTM(units=units,return_sequences = True))\n",
    "model_BiD3.add(GlobalMaxPooling1D())\n",
    "model_BiD3.add(Dense(hidden_dims,activation='relu'))\n",
    "model_BiD3.add(Dropout(0.2))\n",
    "model_BiD3.add(Dense(6,activation='sigmoid'))\n",
    "model_BiD3.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model_BiD3.summary()\n",
    "\n",
    "#Fit model\n",
    "history = model_BiD3.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 3.0, units = 50, coefficient = 9, highest val_acc = 0.9725, val_loss = 0.0702, after 1 epoch\n",
    "\n",
    "Case 3.1.1, units = 149, coefficient = 3, highest val_acc = 0.9721, val_loss = 0.0684, after 2 epochs\n",
    "\n",
    "Case 3.1.2, units = 74, coefficient = 6,highest val_acc = 0.9720, val_loss = 0.0701, after 2 epochs\n",
    "\n",
    "50 is the optimal value, and we will use it for further test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3xTQT3IBoCkx"
   },
   "source": [
    "<a id='s32'></a>\n",
    "### 3.2 Density Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Id_XEWyOn09M"
   },
   "source": [
    "case 3.2.1: For case 3.0 delete middle density layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PSv4wvDCWUo6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_40 (Embedding)     (None, 100, 256)          5120000   \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 100, 50)           61400     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_38 (Glo (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 5,181,706\n",
      "Trainable params: 5,181,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhome/cs/linyy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159571 samples, validate on 63978 samples\n",
      "Epoch 1/5\n",
      "159571/159571 [==============================] - 687s 4ms/step - loss: 0.0726 - accuracy: 0.9770 - val_loss: 0.0721 - val_accuracy: 0.9706\n",
      "Epoch 2/5\n",
      "159571/159571 [==============================] - 679s 4ms/step - loss: 0.0462 - accuracy: 0.9830 - val_loss: 0.0647 - val_accuracy: 0.9737\n",
      "Epoch 3/5\n",
      "159571/159571 [==============================] - 679s 4ms/step - loss: 0.0406 - accuracy: 0.9847 - val_loss: 0.0713 - val_accuracy: 0.9708\n",
      "Epoch 4/5\n",
      "159571/159571 [==============================] - 674s 4ms/step - loss: 0.0363 - accuracy: 0.9861 - val_loss: 0.0741 - val_accuracy: 0.9702\n",
      "Epoch 5/5\n",
      "159571/159571 [==============================] - 670s 4ms/step - loss: 0.0328 - accuracy: 0.9872 - val_loss: 0.0746 - val_accuracy: 0.9715\n"
     ]
    }
   ],
   "source": [
    "units = 50\n",
    "\n",
    "model_BiD5 = Sequential()\n",
    "model_BiD5.add(Embedding(vocab_size, emd_size, input_length=max_length,))\n",
    "model_BiD5.add(LSTM(units=units,return_sequences = True))\n",
    "model_BiD5.add(GlobalMaxPooling1D())\n",
    "model_BiD5.add(Dropout(0.2))\n",
    "model_BiD5.add(Dense(6,activation='sigmoid'))\n",
    "model_BiD5.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model_BiD5.summary()\n",
    "\n",
    "#Fit model\n",
    "history = model_BiD5.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 3.2.1, we remove the density layer from case 3.0, which units = 50, and has the highest val_acc 0.9725. Then in case 3.2.1, highest val_acc = 0.9737. Also similar in textCNN, when units is not that so large, more density layers make no improvement in our case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all tests, the best LSTM model structure is like, units = 50, with just one density layer in the end. The highest val_acc = 0.9737, after 2 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jlQ3-bPJCpMD"
   },
   "source": [
    "<a id='p24'></a>\n",
    "## 4.Attention Models\n",
    "It's not covered in the lecture but since the release of Hierarchical Attention Networks for Document Classification paper written jointly by CMU and Microsoft guys in 2016, it's been quite popular.\n",
    "But what is the REAL incentive after trying this model?\n",
    "It's from the REAL TRUMP: \"what do you have to lose? I say, take it.\", so here I will give it a shoot as the president does to the hydroxychloroquine.\n",
    "\n",
    "As below is a simple attention model which help us by pay more attention to some word since toxic comments tend to be determined by just one or two toxic words, especially some 4 letter word, u know.\n",
    "\n",
    "Obviously, attention can be implemented together with models mentioned above, but since I don't have such time, I will leave it to my partner to do some trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 751,
     "status": "error",
     "timestamp": 1588548949195,
     "user": {
      "displayName": "Yuyang Lin",
      "photoUrl": "",
      "userId": "11037694261943317117"
     },
     "user_tz": -480
    },
    "id": "Ocgy7sGSCpMD",
    "outputId": "1110de4e-3551-42f7-aa95-9cfaaece1e92"
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/qqgeogor/keras-lstm-attention-glove840b-lb-0-043\n",
    "from keras.layers import Layer\n",
    "from keras import backend as K\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(name='{}_W'.format(self.name),\n",
    "                                 shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(name='{}_b'.format(self.name),\n",
    "                                     shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "        \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        e = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))  # e = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            e += self.b\n",
    "        e = K.tanh(e)\n",
    "\n",
    "        a = K.exp(e)\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "\n",
    "        c = K.sum(a * x, axis=1)\n",
    "        return c\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.features_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "case 4.0: implemeted Attention with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 900,
     "status": "error",
     "timestamp": 1588548942166,
     "user": {
      "displayName": "Yuyang Lin",
      "photoUrl": "",
      "userId": "11037694261943317117"
     },
     "user_tz": -480
    },
    "id": "9N55nlFGCpMF",
    "outputId": "3aa845ce-5560-4a06-c773-44a1529a85a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_48\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_48 (Embedding)     (None, 100, 256)          5120000   \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 100, 50)           61400     \n",
      "_________________________________________________________________\n",
      "attention_4 (Attention)      (None, 50)                150       \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 64)                3264      \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 5,185,204\n",
      "Trainable params: 5,185,204\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhome/cs/linyy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159571 samples, validate on 63978 samples\n",
      "Epoch 1/5\n",
      "159571/159571 [==============================] - 704s 4ms/step - loss: 0.0734 - accuracy: 0.9762 - val_loss: 0.0698 - val_accuracy: 0.9726\n",
      "Epoch 2/5\n",
      "159571/159571 [==============================] - 676s 4ms/step - loss: 0.0464 - accuracy: 0.9828 - val_loss: 0.0700 - val_accuracy: 0.9722\n",
      "Epoch 3/5\n",
      "159571/159571 [==============================] - 670s 4ms/step - loss: 0.0401 - accuracy: 0.9846 - val_loss: 0.0795 - val_accuracy: 0.9687\n",
      "Epoch 4/5\n",
      "159571/159571 [==============================] - 666s 4ms/step - loss: 0.0345 - accuracy: 0.9863 - val_loss: 0.0777 - val_accuracy: 0.9701\n",
      "Epoch 5/5\n",
      "159571/159571 [==============================] - 661s 4ms/step - loss: 0.0300 - accuracy: 0.9881 - val_loss: 0.0812 - val_accuracy: 0.9716\n"
     ]
    }
   ],
   "source": [
    "units = 50 # optimal from previous\n",
    "\n",
    "model_a = Sequential()\n",
    "model_a.add(Embedding(vocab_size, emd_size, input_length=max_length,))\n",
    "model_a.add(LSTM(units=units,return_sequences = True))\n",
    "model_a.add(Attention(max_length))\n",
    "#model_a.add(GlobalMaxPooling1D())\n",
    "model_a.add(Dense(hidden_dims,activation='relu'))\n",
    "model_a.add(Dropout(0.2))\n",
    "model_a.add(Dense(6,activation='sigmoid'))\n",
    "model_a.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model_a.summary()\n",
    "\n",
    "#Fit model\n",
    "history = model_a.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y3GUjjllbXiq"
   },
   "source": [
    "case 4.1: Try model structure introduced in https://www.kaggle.com/sanket30/cudnnlstm-lstm-99-accuracy, which is said to be 99% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DSu5mfPsaq86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_64\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_53 (Embedding)     (None, 100, 256)          5120000   \n",
      "_________________________________________________________________\n",
      "bidirectional_23 (Bidirectio (None, 100, 256)          394240    \n",
      "_________________________________________________________________\n",
      "bidirectional_24 (Bidirectio (None, 100, 128)          164352    \n",
      "_________________________________________________________________\n",
      "attention_11 (Attention)     (None, 128)               228       \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 5,679,594\n",
      "Trainable params: 5,679,594\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhome/cs/linyy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159571 samples, validate on 63978 samples\n",
      "Epoch 1/5\n",
      "159571/159571 [==============================] - 1925s 12ms/step - loss: 0.1401 - accuracy: 0.9635 - val_loss: 0.1377 - val_accuracy: 0.9627\n",
      "Epoch 2/5\n",
      "159571/159571 [==============================] - 1894s 12ms/step - loss: nan - accuracy: 0.5738 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 3/5\n",
      "159571/159571 [==============================] - 1896s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 4/5\n",
      "159571/159571 [==============================] - 1891s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 5/5\n",
      "159571/159571 [==============================] - 1882s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM, Dense, Bidirectional, Input,Dropout,BatchNormalization, CuDNNGRU, CuDNNLSTM\n",
    "\n",
    "model_a2 = Sequential()\n",
    "model_a2.add(Embedding(vocab_size, emd_size, input_length=max_length,))\n",
    "model_a2.add(Bidirectional(LSTM(128, dropout=0.4, recurrent_dropout=0.4, activation='relu', return_sequences=True)))\n",
    "model_a2.add(Bidirectional(LSTM(64, return_sequences = True)))\n",
    "model_a2.add(Attention(max_length))\n",
    "model_a2.add(Dense(6,activation='sigmoid'))\n",
    "model_a2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_a2.summary())\n",
    "\n",
    "#Fit model\n",
    "history = model_a2.fit(x_train,y_train, epochs=epochs,validation_data=(x_test,y_test),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "oops, there is something wrong with the result and we talked about it in the Limitation part of our PDF report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.BERT\n",
    "As mentioned in the comment, \"it's not difficult to implement BERT\",so BERT is not implemented. In addition, you may not want to download pretrained model which takes a while through streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='p3'></a>\n",
    "# Test Performance of One Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the acc of each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================= Evaluate toxic ===============================\n",
      "Accuracy in train set of toxic is 0.914113466732677\n",
      "Accuracy in test set of toxic is 0.9123292381756228\n",
      "======================= Evaluate severe_toxic ===============================\n",
      "Accuracy in train set of severe_toxic is 0.9922166308414436\n",
      "Accuracy in test set of severe_toxic is 0.9919972490543625\n",
      "======================= Evaluate obscene ===============================\n",
      "Accuracy in train set of obscene is 0.9569094634990067\n",
      "Accuracy in test set of obscene is 0.9496701991309513\n",
      "======================= Evaluate threat ===============================\n",
      "Accuracy in train set of threat is 0.988926559337223\n",
      "Accuracy in test set of threat is 0.9902779080308857\n",
      "======================= Evaluate insult ===============================\n",
      "Accuracy in train set of insult is 0.9597420583940691\n",
      "Accuracy in test set of insult is 0.9528900559567351\n",
      "======================= Evaluate identity_hate ===============================\n",
      "Accuracy in train set of identity_hate is 0.9865138402341278\n",
      "Accuracy in test set of identity_hate is 0.9847916471287005\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = model_cnn7 # replace with the model want to test\n",
    "\n",
    "for label in list_classes:\n",
    "    print('======================= Evaluate {} ==============================='.format(label))\n",
    "    \n",
    "    y_train_true = train[label]\n",
    "    y_test_true = test_label[label]\n",
    "    \n",
    "    # compute accuracy\n",
    "    y_train_pre = model.predict(x_train)\n",
    "    y_test_pre = model.predict(x_test)\n",
    "    \n",
    "    print('Accuracy in train set of {} is {}'.format(label, accuracy_score(y_train_true, y_train_pre[:,1].round(),  normalize=True)))\n",
    "    print('Accuracy in test set of {} is {}'.format(label, accuracy_score(y_test_true, y_test_pre[:,1].round(),  normalize=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick 10 random sentences from test set and print out the predicted label and true label. \n",
    "\n",
    "And here we use all the sentences in test set, even if their labels are all '-1', which marked as unmarked for this competition, but for test we can still test it. (for labels with all '-1', we considered it as 'unlabelled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./input/train.csv')\n",
    "test = pd.read_csv('./input/test.csv')\n",
    "test_label = pd.read_csv('./input/test_labels.csv')\n",
    "\n",
    "sen_train = train[\"comment_text\"]\n",
    "sen_test = test[\"comment_text\"]\n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y_train = train[list_classes].values\n",
    "y_test = test_label[list_classes].values\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(list(sen_train))\n",
    "tokenized_train = tokenizer.texts_to_sequences(sen_train)\n",
    "tokenized_test = tokenizer.texts_to_sequences(sen_test)\n",
    "\n",
    "x_train = pad_sequences(tokenized_train, maxlen=max_length)\n",
    "x_test = pad_sequences(tokenized_test, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence id is e33928dae1712a06\n",
      "True labels are ['unlabelled']\n",
      "Predicted labels are ['toxic', 'obscene', 'insult']\n",
      "(predicted array [[0.99993944 0.40892202 0.998252   0.0112919  0.86991    0.07564162]])\n",
      "=============================================================\n",
      "Sentence id is 4d8870314a57d8e4\n",
      "True labels are ['unlabelled']\n",
      "Predicted labels are ['normal']\n",
      "(predicted array [[3.9631287e-03 2.3208251e-05 2.4333235e-04 2.5662528e-06 1.8232784e-04\n",
      "  3.5119509e-05]])\n",
      "=============================================================\n",
      "Sentence id is cfbc476bf15f6373\n",
      "True labels are ['unlabelled']\n",
      "Predicted labels are ['normal']\n",
      "(predicted array [[4.2877709e-05 2.9629155e-06 6.6561275e-05 2.0524837e-09 2.0775164e-05\n",
      "  3.8602158e-07]])\n",
      "=============================================================\n",
      "Sentence id is 2c66b6c902a599b2\n",
      "True labels are ['toxic', 'obscene', 'insult']\n",
      "Predicted labels are ['toxic', 'obscene']\n",
      "(predicted array [[9.9888498e-01 3.8344768e-04 9.6542639e-01 6.0155251e-08 7.4289426e-02\n",
      "  5.5028549e-06]])\n",
      "=============================================================\n",
      "Sentence id is f83464b600f0a8ff\n",
      "True labels are ['normal']\n",
      "Predicted labels are ['normal']\n",
      "(predicted array [[3.3287317e-05 1.7350229e-06 2.1727858e-05 3.8528283e-09 1.5490934e-05\n",
      "  2.2218006e-07]])\n",
      "=============================================================\n",
      "Sentence id is 58229f234ff97257\n",
      "True labels are ['normal']\n",
      "Predicted labels are ['normal']\n",
      "(predicted array [[1.0685805e-03 2.1018224e-05 2.5942622e-04 8.5192448e-07 1.7936267e-04\n",
      "  1.5834166e-05]])\n",
      "=============================================================\n",
      "Sentence id is 9c7ff38e0c3509eb\n",
      "True labels are ['normal']\n",
      "Predicted labels are ['normal']\n",
      "(predicted array [[5.3829626e-05 1.2265929e-06 3.2631953e-05 4.8090274e-09 1.3618286e-05\n",
      "  6.2214770e-07]])\n",
      "=============================================================\n",
      "Sentence id is 65461ffec04265a8\n",
      "True labels are ['normal']\n",
      "Predicted labels are ['normal']\n",
      "(predicted array [[3.7148993e-02 1.1006983e-04 1.2876887e-03 2.4636434e-05 1.4574642e-03\n",
      "  1.1218167e-04]])\n",
      "=============================================================\n",
      "Sentence id is 824f1acdf1b0b79b\n",
      "True labels are ['unlabelled']\n",
      "Predicted labels are ['normal']\n",
      "(predicted array [[7.9284509e-05 1.7460379e-06 2.8466993e-05 2.0216198e-08 2.3868315e-05\n",
      "  6.6301527e-07]])\n",
      "=============================================================\n",
      "Sentence id is 489a1be2674f0522\n",
      "True labels are ['normal']\n",
      "Predicted labels are ['toxic']\n",
      "(predicted array [[5.0894004e-01 3.2875780e-03 3.5486403e-01 2.3581835e-05 9.3275383e-03\n",
      "  5.4456718e-04]])\n",
      "=============================================================\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "randoms = random.sample(range(len(x_test)), 10)\n",
    "\n",
    "model = model_cnn7 # replace with the model want to test\n",
    "\n",
    "for i in randoms:\n",
    "\n",
    "        x_sample = x_test[i]\n",
    "        y_true = y_test[i]\n",
    "        y_pre = model.predict(np.array([x_sample,]))\n",
    "        print('Sentence id is {}'.format(test['id'][i]))\n",
    "        label_true = []\n",
    "        label_pre = []\n",
    "        for i in range (len(list_classes)):\n",
    "            if (y_true[i] == 1):\n",
    "                label_true.append(list_classes[i])\n",
    "            if (y_pre[0][i] >= 0.5):\n",
    "                label_pre.append(list_classes[i])\n",
    "        if (y_true[0] == -1):\n",
    "            label_true.append('unlabelled')\n",
    "            \n",
    "        if (len(label_true) == 0):\n",
    "            label_true.append('normal')\n",
    "            \n",
    "        if (len(label_pre) == 0):\n",
    "            label_pre.append('normal')\n",
    "        print('True labels are {}'.format(label_true))\n",
    "        print('Predicted labels are {}'.format(label_pre))\n",
    "        print('(predicted array {})'.format(y_pre))\n",
    "        print('=============================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also test the model with any sentence you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence is hello world\n",
      "Predicted labels are ['normal']\n",
      "(predicted array [[2.9118722e-03 2.1204372e-05 1.8908845e-03 6.1425867e-06 7.1677449e-04\n",
      "  3.2186953e-04]])\n"
     ]
    }
   ],
   "source": [
    "model = model_BiD3; # replace the model want to test\n",
    "\n",
    "sen_one_test = \"hello world\"\n",
    "\n",
    "tokenized_one_test = tokenizer.texts_to_sequences([sen_one_test])\n",
    "\n",
    "x_one_test = pad_sequences(tokenized_one_test, maxlen=max_length)\n",
    "\n",
    "y_pre = model.predict(np.array(x_one_test)) \n",
    "\n",
    "label_pre = []\n",
    "for i in range (len(list_classes)):\n",
    "    if (y_pre[0][i] >= 0.5):\n",
    "        label_pre.append(list_classes[i])\n",
    "            \n",
    "    if (len(label_pre) == 0):\n",
    "        label_pre.append('normal')\n",
    "\n",
    "print('Sentence is {}'.format(sen_one_test))\n",
    "print('Predicted labels are {}'.format(label_pre))\n",
    "print('(predicted array {})'.format(y_pre))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "comp3359proj.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
